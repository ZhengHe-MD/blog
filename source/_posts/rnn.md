---
title: ä»å¤´å¼€å§‹å®ç° RNN
date: 2022-02-20 15:03:20
tags:
---

> What I cannot create, I do not understand. -- Richard Feynman

Andrej Karpathy åœ¨ 2015 å¹´å‘è¡¨äº†é¢˜ä¸º [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) çš„åšå®¢ï¼Œå¹¶é…å¥—å¼€æºäº†å…¶ä¸­å®éªŒæ‰€ç”¨çš„ [char-rnn ä»£ç ä»“åº“](https://github.com/karpathy/char-rnn)ï¼Œä»¥åŠç”¨ numpy æ‰‹å†™çš„ [gist: min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086)ï¼Œé˜…è¯»è¿‡åå—ç›Šè‰¯å¤šã€‚äºæ˜¯æœ€è¿‘èŠ±äº†ä¸€äº›æ—¶é—´åšäº†ä¸‹é¢è¿™äº›äº‹æƒ…ï¼š

1. é€è¡Œç†è§£ min-char-rnnï¼Œå³ vanilla RNN
2. å®ç° N å±‚ vanilla RNN
3. å®ç° LSTM (Long Short-Term Memory) RNN
4. æ¢ç´¢ RNN çš„å¯èƒ½æ€§

   * Paul Graham generator
   * ä¸‰å›½æ¼”ä¹‰
   * è€å‹è®°
   * Kubernetes æºç 
   * è¶…çº§ä¸¹çš„æŠ€æˆ˜æœ¯

æ•´ä¸ªæ¢ç´¢è¿‡ç¨‹å¯¹æˆ‘è€Œè¨€å……æ»¡äº†è¶£å‘³å’ŒæŒ‘æˆ˜ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­ä»¤äººæ¿€åŠ¨åœ°é¦–æ¬¡ä½¿ç”¨å¾®ç§¯åˆ†çš„çŸ¥è¯†ã€‚å°½ç®¡è¿™åªæ˜¯æ·±åº¦å­¦ä¹ çš„å†°å±±ä¸€è§’ï¼Œä½†è¶³ä»¥è®©ä¸€åä¸»è¥ä¸šåŠ¡ä¸ºæœåŠ¡ç«¯å¼€å‘çš„è½¯ä»¶å·¥ç¨‹å¸ˆæ„Ÿåˆ°æ¿€åŠ¨ä¸å·²ï¼Œäºæ˜¯ä¾¿æœ‰äº†è¿™ç¯‡åšå®¢ï¼Œå°†è¿™ä¸ªè¿‡ç¨‹è®°å½•ä¸‹æ¥ã€‚

<!-- more -->

åœ¨æ„æ€è¿™ç¯‡åšå®¢å‰ï¼Œæˆ‘æ›¾ç»æƒ³è¿‡å†™ä¸€ä¸ªå®Œæ•´çš„ã€ç»†è‡´å…¥å¾®çš„ã€ä¿å§†å¼çš„ä» 0 - 1 å®ç° RNN çš„æ•™ç¨‹ã€‚åæ¥å‘ç°ï¼Œåœ¨æˆ‘è‡ªå·±é˜…è¯»ã€ç†è§£å’Œå®ç°çš„è¿‡ç¨‹ä¸­ï¼Œè‡³å°‘æŠ•å…¥æ•°åä¸ªå°æ—¶é˜…è¯»å¤§é‡èµ„æ–™ï¼ŒæœŸé—´å¹¶æ²¡æœ‰å‘ç°ä»»ä½•ä¸€ç¯‡æ–‡ç« èƒ½åšåˆ°è¿™ç‚¹ï¼Œé‚£ä¹ˆæˆ‘åˆå¦‚ä½•èƒ½æœŸæœ›å†™ä¸€ç¯‡åšå®¢åšåˆ°è¿™ç‚¹ï¼Ÿ

![xkcd-standards](https://imgs.xkcd.com/comics/standards.png)

é‚£ä»€ä¹ˆä¸œè¥¿æ˜¯å€¼å¾—åˆ†äº«çš„å‘¢ï¼Ÿæˆ‘æ€è€ƒçš„ç»“æœæ˜¯ã€Œå­¦ä¹ è·¯å¾„ã€ã€‚åœ¨é‡åˆ°ç†è§£éšœç¢æ—¶ï¼Œæˆ‘å°è¯•å¯»æ‰¾å¹¶é˜…è¯»äº†å“ªäº›èµ„æ–™ï¼ŒèŠ±è´¹äº†å¤šé•¿æ—¶é—´ï¼Œæœ‰ä»€ä¹ˆå¿ƒå¾—ä½“ä¼šï¼Œè¿™äº›ä¹Ÿè®¸èƒ½å¤Ÿç»™åˆ°è¯»è€…çµæ„Ÿã€‚

> âš ï¸ æœ¬æ–‡å‡è®¾è¯»è€…æœ‰ä¸€å®šçš„æ·±åº¦å­¦ä¹ ç†è®ºåŸºç¡€å’Œå®è·µç»éªŒï¼ŒåŒæ—¶é˜…è¯»è¿‡ä¸Šè¿°çš„åšå®¢å’Œè„šæœ¬ã€‚ä½†å¦‚æœä¸ä»‹æ„å¯èƒ½å‡ºç°çš„ç†è§£éšœç¢ï¼Œä¹Ÿæ¬¢è¿ç»§ç»­é˜…è¯»æœ¬æ–‡ã€‚
>
> ğŸ’¡ ä¸€ä½çƒ­å¿ƒç½‘å‹å°±ç€ min-char-rnn ç»™å‡ºäº†ä¸€ä¸ªå¸¦æ³¨é‡Šçš„[ç‰ˆæœ¬](https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py)ï¼Œé˜…è¯»åè€…çš„éš¾åº¦ä¼šæ›´å°ä¸€äº›

## ç†è§£ min-char-rnn

min-char-rnn å®ç°çš„æ˜¯å•å±‚çš„ vanilla RNNï¼Œå…¶æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of min-char-rnn](./one-layer.png)

### å‰å‘ä¼ æ’­

ä»¥ä¸‹æ˜¯æºç ä¸­çš„å‰å‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
for t in xrange(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
```

é€šå¸¸æ¯”è¾ƒç²¾å·§çš„åœ°æ–¹å°±æ˜¯ä»¤äººè´¹è§£çš„åœ°æ–¹ï¼Œè¿™é‡Œæ¯”è¾ƒç²¾å·§çš„è‡ªç„¶æ˜¯ CrossEntropy çš„è®¡ç®—ï¼š

```python
loss += -np.log(ps[t][targets[t],0])
```

CrossEntropy è®¡ç®—çš„æ˜¯ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ç¨‹åº¦ï¼Œå·®å¼‚è¶Šå¤§ï¼Œç†µå€¼è¶Šå¤§ã€‚è¿™é‡Œçš„ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒåˆ†åˆ«æ˜¯ã€ŒRNN é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€å’Œã€Œå®é™…ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€ï¼Œåœ¨ç›‘ç£å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œåè€…æ˜¯ä¸€ä»¶å·²ç»ç¡®å®šçš„äº‹æƒ…ï¼Œæ‰€ä»¥å®ƒçš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ [0, 0, ..., 1, 0, 0]ï¼Œå³é™¤ä¸€ä¸ªç¡®å®šå­—ç¬¦çš„å‡ºç°æ¦‚ç‡ä¸º 1 å¤–ï¼Œå‰©ä½™å­—ç¬¦çš„å‡ºç°æ¦‚ç‡çš†ä¸º 0ã€‚äºæ˜¯æœ€ç»ˆå¯¹æŸå¤±å‡½æ•°äº§ç”Ÿè´¡çŒ®çš„åªæœ‰æŸä¸€ä¸ªå­—ç¬¦å‡ºç°çš„æ¦‚ç‡ï¼Œå³è¿™é‡Œçš„ `ps[t][targets[t], 0]`ã€‚

### åå‘ä¼ æ’­

ä»¥ä¸‹æ˜¯æºç ä¸­çš„åå‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
for t in reversed(xrange(len(inputs))):
    dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
    dWhy += np.dot(dy, hs[t].T)
    dby += dy
    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)
```

å¦‚æœä½ åƒæˆ‘ä¸€æ ·åªå¯¹æ•°å­¦åˆ†æä¸­æœ€åŸºæœ¬çš„ã€Œå•å˜é‡æ±‚å¯¼ã€è¿˜ç•™å­˜æœ‰ä¸€å®šçš„è®°å¿†ï¼Œé‚£çœ‹è¿™æ®µä»£ç ä¼šæœ‰ç§ã€Œè¡¨é¢ä¸Šå¥½åƒèƒ½çœ‹æ‡‚ï¼Œä»”ç»†æ€è€ƒåˆè§‰å¾—å“ªé‡Œå¯¹ä¸ä¸Šã€çš„æ„Ÿè§‰ã€‚å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“çœ‹å‡ºæ¥è¿™äº›ä»£ç æ˜¯åœ¨å°†æŸå¤±å‡½æ•°çš„å˜åŒ–æŒ‰ç›¸åçš„æ–¹å‘ä¸€æ­¥ä¸€æ­¥åœ°å€’æ¨å›å»ï¼Œå†åŠ ä¸Šè„‘æµ·ä¸­å°šå­˜çš„å•å˜é‡ã€Œé“¾å¼æ±‚å¯¼ã€è§„åˆ™ï¼Œä¼¼ä¹ä¸€åˆ‡é¡ºç†æˆç« ã€‚ä½†ã€Œç»†æ€ææã€ï¼Œè¿™é‡Œé¢æœ‰æ ‡é‡ã€æœ‰å‘é‡ï¼Œè¿˜æœ‰çŸ©é˜µï¼Œä»€ä¹ˆæ˜¯ã€Œæ ‡é‡å¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€Œæ ‡é‡å¯¹çŸ©é˜µæ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€Œå‘é‡å¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€ŒçŸ©é˜µå¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€ŒçŸ©é˜µå¯¹çŸ©é˜µæ±‚å¯¼ã€ï¼Ÿæˆ‘ä»¬å­¦è¿‡ã€Œå‘é‡æ±‚å¯¼ã€æˆ–è€…ã€ŒçŸ©é˜µæ±‚å¯¼ã€å—ï¼Ÿ

ä¸ºäº†æ¶ˆè§£è„‘æµ·ä¸­å°šä¸æ¸…æ™°çš„åœ°æ–¹ï¼Œæˆ‘é€šè¿‡æœç´¢å¼•æ“ï¼Œä»¥ç±»ä¼¼ã€Œmatrix calculus for engineerã€çš„å…³é”®è¯ï¼Œæ‰¾åˆ°äº†ä¸€ç¯‡é•¿æ–‡ï¼š[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)ï¼Œå®ƒæ­£å¥½æ˜¯ä¸ºåƒæˆ‘è¿™æ ·**åªè®°å¾—ã€Œå•å˜é‡æ±‚å¯¼ã€çš„äºº**é‡èº«å®šåšçš„èµ„æ–™ã€‚äºæ˜¯æˆ‘é›†ä¸­ç²¾åŠ›èŠ±äº† 3-4 ä¸ªå°æ—¶æŠŠå…¨æ–‡é€šè¯»ï¼Œéšååœ¨ç¬”è®°æœ¬ä¸Šå°†ä»£ç ä¸­çš„å‡ ä¸ªæ ‡é‡ (æŸå¤±å‡½æ•°)ã€å‘é‡ã€çŸ©é˜µé—´çš„æ±‚å¯¼éƒ½æ¨äº†ä¸€éï¼Œç»ˆäºäº‘å¼€é›¾æ•£ï¼Œäº‹å®è¯æ˜è¿™æ ·çš„ä»˜å‡ºååˆ†å€¼å¾—ã€‚

Andrej åœ¨åå‘ä¼ æ’­ Softmax + CrossEntropy æ—¶å†™äº†ä¸ªå¤‡æ³¨ï¼š

> backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here

ä»¤äººæ²®ä¸§çš„æ˜¯ï¼Œå½“ä½ ä»”ç»†é˜…è¯»è¿™éƒ¨åˆ†å†…å®¹ï¼Œä¼šå‘ç°è¿™æ ·ä¸€å¥è¯ï¼š

> Itâ€™s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out...

çœŸæ­£çœ‹æ‡‚å®ƒï¼Œæ˜¯æˆ‘åœ¨é˜…è¯»å®Œä¸Šé¢é‚£ç¯‡å…³äº Matrix Calculus çš„æ‰«ç›²æ–‡ç« ï¼Œç´§æ¥ç€ç»§ç»­é˜…è¯»è¿™ç¯‡åšå®¢ [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) ä¹‹åã€‚**æœ€ç»ˆçš„ç­”æ¡ˆå¾ˆå…·æœ‰ç¾æ„Ÿï¼Œä½†æ¨å¯¼çš„è¿‡ç¨‹å¾ˆéœ€è¦è€å¿ƒ**ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™é‡Œåœ¨è®­ç»ƒçš„æ—¶å€™è¿˜ä½¿ç”¨äº†ã€Œteacher forcingã€çš„æŠ€å·§ï¼š

```python
dy = np.copy(ps[t])
dy[targets[t]] -= 1
```

å³ä¸ç®¡ RNN åœ¨æ¯æ¬¡ unroll éƒ½ä½¿ç”¨æ ‡å‡†ç­”æ¡ˆï¼Œè€Œä¸æ˜¯è‡ªèº«å­¦åˆ°çš„ç»“æœ `np.argmax(ps[t])`ã€‚åœ¨å…¶å®ƒæ•™ç¨‹ä¸­ï¼Œæˆ‘ä¹Ÿçœ‹åˆ°è¿‡äººä»¬ä¼šç»™ã€Œteacher forcingã€æ–½åŠ ä¸€ä¸ªæ¦‚ç‡ï¼Œå°±åƒ `dropout_p`ï¼Œå› ä¸ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ä¼šæœ‰ä»»ä½•å­—ç¬¦çš„å‚è€ƒç»“æœã€‚æ®è¯´é€šè¿‡é™ä½è¿™ä¸ªæ¦‚ç‡å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šé¿å…ã€Œå­¦ç”Ÿåœ¨å®é™…è§£é¢˜æ—¶è¿‡åˆ†ä¾èµ–è€å¸ˆã€ã€‚

### Gradient check

åœ¨ min-char-rnn è„šæœ¬ä¸‹çš„ç¬¬ä¸€ä¸ªè¯„è®ºï¼Œå°±æ˜¯ Andrej è‡ªå·±çš„ï¼š

![gradient check](./gradient-check.png)

åˆšçœ‹åˆ°è¿™æ®µä»£ç ï¼Œæˆ‘å®Œå…¨ä¸çŸ¥é“ gradient check æ˜¯åœ¨åšä»€ä¹ˆï¼Œè‡ªç„¶ä¹Ÿæ— æ³•ç†è§£è¿™æ®µä»£ç çš„å«ä¹‰ã€‚ä¸è¿‡åœ¨æœç´¢å¼•æ“çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘è¿˜æ˜¯æ‰¾åˆ°äº†æ–¯å¦ç¦å¤§å­¦å¼€æ”¾çš„æ•™ç¨‹ [Unsupervised Feature Learning and Deep Learning](http://ufldl.stanford.edu/tutorial/)ï¼Œå…¶ä¸­ä¸€èŠ‚æ­£æ˜¯ä»‹ç» [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)ã€‚æ‰€è°“ gradient check(ing) å…¶å®å°±æ˜¯å¯¹æ¯”ã€Œé€šè¿‡åå‘ä¼ æ’­è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦ã€ä¸ã€Œåˆ©ç”¨å¯¼æ•°å®šä¹‰è¿‘ä¼¼è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ã€ï¼Œæ¥åˆ¤æ–­è‡ªå·±çš„åå‘ä¼ æ’­é˜¶æ®µä»£ç æ˜¯å¦å†™å¯¹äº†ï¼Œæ˜¯å·¥ç¨‹å¸ˆæ„å»ºç¥ç»ç½‘ç»œæ—¶å¸¸ç”¨çš„ä¸€ç§ç®€å•æœ‰æ•ˆçš„ debug æ–¹æ³•ã€‚ç”±äºæ¯æ¬¡è®­ç»ƒéƒ½å¯èƒ½éå¸¸è€—æ—¶ï¼Œè€Œä¸”ä¸€äº›é—®é¢˜å³ä¾¿å­˜åœ¨ï¼Œè¡¨é¢ä¸Šä¹Ÿå¯èƒ½çœ‹ç€ä¸€åˆ‡æ­£å¸¸ã€‚åœ¨æ„å»ºå®Œç¥ç»ç½‘ç»œåï¼Œå¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæ‰§è¡Œ gradient check(ing) å¾ˆæœ‰å¿…è¦ã€‚

åœ¨å®ç° 2 å±‚ã€ n å±‚ RNN çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å°±åˆ©ç”¨ gradient check(ing) æˆåŠŸå‘ç°äº†ä»£ç ä¸­çš„è‹¥å¹²é€»è¾‘é—®é¢˜ã€‚è¿™äº›é—®é¢˜ä¸ä¼šå¯¼è‡´ç¨‹åºå´©æºƒï¼Œé€šè¿‡ä¼ ç»Ÿçš„ã€Œçœ¼ç¥è°ƒè¯•ã€ã€ã€Œprint è°ƒè¯•ã€ã€ã€Œå•ç‚¹è°ƒè¯•ã€éƒ½æéš¾å‘ç°ã€‚

## N å±‚ vanilla RNN

åœ¨é˜…è¯» min-char-rnn æ—¶ï¼Œæˆ‘åœ¨å¿ƒé‡Œå°±èŒç”Ÿäº†ä¸€ä¸ªæƒ³æ³•ï¼šèƒ½å¦ä¾è‘«èŠ¦ç”»ç“¢ç›´æ¥ç”¨ numpy å®ç°ä¸€ä¸ª N å±‚ vanilla RNNï¼Ÿä½†æ­¥å­å¤ªå¤§å®¹æ˜“æ‰¯ç€è›‹ï¼Œå…ˆæŒ‘æˆ˜ä¸€ä¸ª 2 å±‚çš„ vanilla RNNï¼š

![The architecture of a 2-layer RNN](./two-layer.png)

åœ¨ç»“æ„ä¸Šï¼Œ2 å±‚çš„ vanilla RNN å¤šäº†ä¸€ä¸ª hidden layerï¼Œç¬¬äºŒå±‚ (h2) çš„è¾“å…¥å°±æ˜¯ç¬¬ä¸€å±‚ (h1) çš„è¾“å‡ºï¼Œä¸»è¦åŒºåˆ«åœ¨äºä¸¤å±‚çš„è¾“å…¥ç»´åº¦ä¸åŒï¼Œå› ä¸ºè¿™ç‚¹ä¸åŒï¼Œåœ¨ä»£ç ä¸­éœ€è¦åˆ†å¼€å®ç°å®ƒä»¬çš„ä¼ æ’­é€»è¾‘ã€‚æˆ‘çš„å®ç°å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_two_layers.py)æ‰¾åˆ°ï¼Œä»£ç ä¸­çš„å˜é‡å‘½åä¸å›¾ä¸­çš„æ ‡è¯†ä¿æŒä¸€è‡´ã€‚å®ç°åŸºæœ¬é€»è¾‘å¤§çº¦èŠ±è´¹äº†åŠä¸ªå°æ—¶æ—¶é—´ï¼Œä½†ç”±äºå¯¹ numpy çš„ä¸ç†Ÿæ‚‰ä»¥åŠä¸€äº›çŸ©é˜µçš„åå­—å’Œç»“æ„ç›¸è¿‘å¯¼è‡´çš„ typoï¼Œä¸€å¼€å§‹å¹¶ä¸æ˜¯ bug-freeã€‚è¿˜å¥½åœ¨é˜…è¯» min-char-rnn çš„æ—¶å€™æ²¡æœ‰å·æ‡’å¿½ç•¥ gradient check(ing)ï¼Œå¤šäºå®ƒå‘Šè¯‰æˆ‘å®ç°æœ‰è¯¯ï¼Œé¿å…è®­ç»ƒå‡ºäººå·¥æ™ºéšœã€‚

æœ‰äº†ä¸Šé¢çš„åŸºç¡€ï¼Œä» 2 å±‚åˆ° N å±‚çš„è¿‡ç¨‹å°±æ¯”è¾ƒèƒ¸æœ‰æˆç«¹ã€‚N å±‚ çš„ vanilla RNN ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of a n-layer RNN](./n-layer.png)

ç»“æ„ä¸Šï¼Œç¬¬äºŒå±‚ (h2) åˆ°ç¬¬ N å±‚ (hn) çš„ä¼ æ’­é€»è¾‘ä¸€æ ·ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ä¸€ä¸ªå¾ªç¯æ¥å®ç°ã€‚æœ‰äº†å‰é¢çš„ç»éªŒï¼Œè¿™æ¬¡å¾ˆå¿«å°±é€šè¿‡äº† gradient check(ing)ï¼Œæˆ‘çš„å®ç°å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_n_layers.py)æ‰¾åˆ°ï¼Œä»£ç ä¸­çš„å˜é‡å‘½åä¸å›¾ä¸­çš„æ ‡è¯†ä¿æŒä¸€è‡´ã€‚

## LSTM

ä» vanilla RNN åˆ° LSTM çš„è¿‡ç¨‹æ¯”æƒ³è±¡ä¸­æ›´è‰°éš¾ã€‚ç»è¿‡ç®€å•çš„å…³é”®è¯æ£€ç´¢ï¼Œä¸éš¾å‘ç° [nicodjimenez/lstm](https://github.com/nicodjimenez/lstm/) é¡¹ç›®ï¼Œç®€å•æ‰«ä¸€çœ¼å®ƒçš„ READMEï¼š

> A basic lstm network can be written from scratch in a few hundred lines of python, yet most of us have a hard time figuring out how lstm's actually work. The original Neural Computation paper is too technical for non experts. Most blogs online on the topic seem to be written by people who have never implemented lstm's for people who will not implement them either. Other blogs are written by experts ...

åŸºæœ¬å¯ä»¥æ–­å®šè¿™æ­£æ˜¯æˆ‘æ‰€éœ€ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œä½œè€…æåˆ°ä¸€ç¯‡ç»¼è¿°è®ºæ–‡ï¼š[A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/abs/1506.00019)ï¼Œç§°èµè¿™ç¯‡æ–‡ç« çš„åŒæ—¶ä¹Ÿæåˆ°è‡ªå·±çš„[å®ç°](https://github.com/nicodjimenez/lstm/blob/master/lstm.py)ä½¿ç”¨äº†æ–‡ç« ä¸­çš„æ•°å­¦ç¬¦å·ã€‚éƒ½å·²ç»çœ‹äº†ä¸€ç¯‡ matrix calculusï¼Œå†çœ‹ä¸€ç¯‡ç»¼è¿°æ–‡ç« é‚£åˆå¦‚ä½•ï¼Ÿäºæ˜¯åˆæ˜¯ä¸‰å››ä¸ªå°æ—¶è¿‡å»äº†...

è¯»æ¯•ï¼Œæˆ‘æœ€å¤§çš„æ„Ÿå—æ˜¯ï¼šæ¯”ã€Œç›´æ¥å­¦ä¹ å®ƒçš„ç»“æ„ï¼Œç„¶åå®ç°å®ƒã€æ›´é‡è¦çš„æ˜¯ç†è§£å®ƒçš„æå‡ºæ˜¯ä¸ºäº†è§£å†³ä»€ä¹ˆé—®é¢˜ã€‚å¦‚æœæ—¶é—´ä¸å¤Ÿï¼Œä¹Ÿå¯ä»¥é˜…è¯»è¿™ç¯‡åšå®¢ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)ã€‚

> ğŸ’¡åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œå°è¯•è‡ªå·±ç”¨ç¬”å’Œçº¸ç”»ä¸€äº› LSTM çš„ç»“æ„æœ‰åŠ©äºå¯¹ç½‘ç»œç»“æ„æ•´ä½“çš„ç†è§£

æœ‰äº†ä¸Šé¢çš„å‚¨å¤‡ï¼Œæ¥ä¸‹æ¥å°±å›åˆ° nicodjimenez/lstmï¼ŒæŠŠè„šæœ¬çœ‹æ‡‚å³å¯ã€‚äº‹ååˆ†æï¼Œä»æµ‹è¯•ç”¨ä¾‹å‡ºå‘æœ‰åŠ©äºæ›´å¿«åœ°ç†è§£ä»£ç ç»“æ„ã€‚ä½œè€…è‡ªå·±å†™äº†ä¸€ç¯‡åšå®¢ [Simple LSTM](https://nicodjimenez.github.io/2014/08/08/lstm.html) (å‘è¡¨æ—¶é—´æ—©äº Andrej çš„åšå®¢) ä»‹ç»ä»£ç ä¸­çš„åå‘ä¼ æ’­éƒ¨åˆ†ï¼Œä½†åªè¦å½»åº•ç†è§£äº† vanilla RNN çš„åå‘ä¼ æ’­åŸç†ï¼Œè¿™é‡Œçš„æ¨å¯¼åœ¨éš¾åº¦ä¸Šå¹¶æ²¡æœ‰æœ¬è´¨çš„æå‡ï¼Œåªæ˜¯ä¼ æ’­çš„æµå‘æ¯”åè€…å¤æ‚ä¸€äº›ã€‚

åœ¨èŠ±è´¹å°†è¿‘ä¸€ä¸ªå°æ—¶åï¼Œä¾è‘«èŠ¦ç”»ç“¢ï¼Œæˆ‘å†™å‡ºäº† [min-char-rnn çš„ lstm ç‰ˆæœ¬](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_lstm.py)ã€‚åŒæ ·åœ°ï¼Œåˆ©ç”¨ gradient check(ing) ç¡®ä¿åå‘ä¼ æ’­éƒ¨åˆ†çš„é€»è¾‘å®ç°æ­£ç¡®ã€‚

## æ¢ç´¢ RNN çš„å¯èƒ½æ€§

å®ç°å®Œç©å…·ç‰ˆ RNNï¼Œç†è§£å·²ç»è¶³å¤Ÿåˆ°ä½ï¼Œå¯ä»¥å¼€å§‹åšä¸€äº›æœ‰æ„æ€çš„äº‹æƒ…äº†ã€‚æœ¬æ–‡å¼€ç¯‡æåˆ°è¿‡ï¼ŒAndrej å¼€æ”¾äº†ä»–åšå®éªŒæ‰€ä½¿ç”¨çš„ [char-rnn](https://github.com/karpathy/char-rnn) ä»£ç ä»“åº“ï¼Œæä¾›äº† RNNã€LSTM å’Œ GRU (å¦ä¸€ç§ RNN) çš„å®ç°ä»¥åŠä¸€æ•´å¥—è®­ç»ƒã€æ¨ç†çš„æµç¨‹ã€‚åœ¨é¡¹ç›® [README.md](https://github.com/karpathy/char-rnn/blob/master/Readme.md) ä¸­ï¼ŒAndrej åˆæ¨èäº†æ¥è‡ª Justin Johnson ä¼˜åŒ–æ€§èƒ½åçš„ç‰ˆæœ¬ï¼š[torch-rnn](https://github.com/jcjohnson/torch-rnn)ï¼Œåè€…ç”šè‡³æä¾›äº†[å®¹å™¨åŒ–æ”¯æŒ](https://github.com/crisbal/docker-torch-rnn)ã€‚åœ¨èŠ±è´¹äº†è‹¥å¹²å°æ—¶ (å°´å°¬çš„æ˜¯ï¼Œå¤§éƒ¨åˆ†æ—¶é—´èŠ±åœ¨äº†ç­‰å¾… `docker pull` ä¸Š) æ­å»ºå¥½ GPU ç¯å¢ƒåï¼Œç»ˆäºå¯ä»¥å¼€å§‹æ­£å¼æ¢ç´¢ã€‚

> ğŸ’¡å¦‚æœä½ æ‰‹è¾¹æœ‰ GPUï¼Œæ­å»ºç¯å¢ƒæ—¶å¯èƒ½è¿˜éœ€è¦è¿™äº›èµ„æ–™ï¼š
>
> * [cuda-installation-guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)
> * [nvidia-docker2](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
>
> æœ€åç”¨ nvidia-docker å¯åŠ¨æ—¶å¯ä»¥æŠŠ torch-rnn é¡¹ç›®çš„æœ¬åœ°è·¯å¾„ç”¨ Docker çš„ volume å‚æ•°ä¸å®¹å™¨å†…éƒ¨çš„ `~/torch-rnn` è·¯å¾„å»ºç«‹åŒå‘ç»‘å®šï¼Œè¿™æ ·å°±å¯ä»¥åœ¨å®¹å™¨é‡Œè‡ªç”±åœ°è®­ç»ƒæ¨¡å‹ã€‚

### Paul Graham generator

å…ˆå¤ç°ä¸€ä¸‹ Andrej åšå®¢ä¸­çš„ç¬¬ä¸€ä¸ªå®éªŒï¼Œå°† Paul Graham çš„ essays æŠ“å–åˆ°æœ¬åœ° ([æˆ‘çš„è„šæœ¬](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/paulgraham/dump_paulgraham_essays.py))ï¼Œç„¶åå°†å…¶åˆå¹¶æˆä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œé€å…¥æ¨¡å‹ã€‚å®éªŒå‚æ•°ä¸ Andrej çš„ä¿æŒä¸€è‡´ï¼Œè®­ç»ƒ 50 ä¸ª epochsï¼Œä¸‹é¢æ˜¯ç”¨æ¨¡å‹é‡‡æ ·å‡ºçš„æ®µè½ç¤ºä¾‹ï¼š

```
So there even a list of few things you're supposed to concipe of 
different startupsâ€”you need to convince customers that new gets 
painting is one of them at startups without taking that. They're a 
rare concopporation, that's within college. To a real way to addict 
you, but what you're expected.
If you're capped in several programmers, what proves them-defend 
what you're working about. But you don't notice based on your generation: 
since he doesn't realize it: all much shouldn't be just that that, 
that is short of this actual audience. Open study new people 
they have shifting up a city. [ 7 ] This surprises still say here told 
it's a platazine. There seem to degree why formidable things we tend to 
volunteregal startups elity is in Grisk Fortle. Sometomes The 
other generations were proportionate at real implications to cogain.
If there are working off you're working on simply to like that sharp 
it horter than you. How so hote-wealth to treates back of friends 
they to be doing. Most nived in a number is to concied our fioks companies.
```

ä¸­é—´çš„è¿™å¥è¯è®©æˆ‘å¾ˆæ˜¯å–œæ¬¢ï¼š

> To a real way to addict you, but what you're expected.

å¯ä»¥ç¿»è¯‘æˆã€Œè®©ä½ ä¸Šç˜¾ï¼Œç”šè‡³ç”¨ä½ æœŸæœ›çš„æ–¹å¼ã€å—ï¼Ÿ

### ä¸‰å›½æ¼”ä¹‰

ç½—è´¯ä¸­çš„ç‰ˆæƒç°åœ¨å·²ç»ä¸å—æ³•å¾‹ä¿æŠ¤ï¼Œäºæ˜¯æˆ‘è½»è€Œæ˜“ä¸¾åœ°åœ¨ç™¾åº¦æœåˆ°äº†ä¸‰å›½æ¼”ä¹‰çš„å…¨æ–‡ txtã€‚è®©æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹ RNN çš„æ–‡è¨€æ–‡ä½œå“ï¼š

```
å¤ªå²äº‘é•¿é—»ä¹‹ï¼Œå³é£äººé©¬å¾€å…¨å¾èšåŠå…³ç¼šã€‚æ–—æ—¶ä¸€å°†å†›é©¬ï¼Œæ¬²å–å¹³éƒ¡ï¼›
æ“è½»å‹’å¼ƒè¡£å¼“ç”²ï¼Œæ¥å¤œèº«ä¸­ã€‚åŸé©¬ç„å¾·å›å¯¨æ–°ä¸­ï¼Œå‹’æ³•ä¸œé£æœªè¿˜ï¼Œå› è™½å¸¦æ˜ä¸€æ—è€Œåˆ°ã€‚
éŸ¦ä¼—å°†ä»¤å†›æ¥åï¼ŒæŠ˜å£°æ¡¥å‰‘äºç©¿ç€ã€‚å¤‡å…¥åŸåé‡Œï¼Œèµµäº‘æ‹é©¬çš†è¿›ï¼Œè¢«ç»™æ¥è¿ä¹‹ï¼Œ
å¸ƒä¼šç»³å¸è½ä¸‹å‘½ï¼Œå¼•ä¸€é©¬å†›æ— å¼©è€Œèµ°ã€‚ç™¾å‡‰äº”è·¯ï¼Œæ€ä¹‹å¤„ï¼Œèµ°ç”±ç²¾è¥é©¬ä¹Ÿã€‚
æ¬¡æ—¥ï¼Œå¦‚æŸ´æŠŠæ‹¥å±±ç™½ä¸‹é˜µã€‚å¾…æˆ‘ä¸‰æœˆï¼Œæ‰‹åŠ›é¡ºä»ï¼Œç›´è‡³ç™½é…ã€‚ä¸¤åç™½å¾—å°æªç²®ä½ä¹±ï¼Œ
ç æ‹œå†›å£«ç›¸æ‹›çƒˆï¼Œå…´é˜µåæ–™ï¼Œé«˜è¾¹å§æ‰“å®åä¸€é½å‡ºï¼Œå³å›¾å±±è‰ï¼Œå±±è¡Œè‡³æºªã€‚
åˆæ‰€é€šäººæ‰“èº«å¹³é˜µï¼Œå…¼è§å‰¿ä¼™ã€‚é©¬è¶…åœ¨å‰æ‰ä¸‹å…µï¼Œä¸€é¢æœ‰å†›å§æ´›ç™¾äººï¼Œåªè¢«ç™½é¦–è§†ä¹‹ã€‚
æ“ä¸ºå½’é”‹ï¼Œä¸•è´¥å®£æ¡¥è¿›èµ°æ›°ï¼šâ€œå®‹å¤§æµ©äºæ¥æ°´ï¼Œæœªå¦‚èƒ½æ•¢æ–½ï¼›æ— æ­¹æŸ“ä¹‹å…µï¼Œ
å½¼å¾—ç”ŸåŠ¨è°—ä»¥ç†è¾å·±ä¸‹å¤§æœºä¹Ÿï¼â€è¡¨äº‘ï¼šâ€œä¸å¯æ­¤å¸¸ä¹‹ï¼Œä»Šä¸ä½•æ„ï¼â€
éšä»å•æ—·ä»¤è¥¿å‡‰å§œå¸ƒæœ‰è†å·ï¼Œç®­æ”¿è·ªèº«ï¼Œå†›é©¬æŒºå—ã€‚æ›¹æ“å¤§ä¸å‘Šç¦»ï¼Œä½•å®ˆè€Œé™ã€‚å…ˆä¸»éå£«ï¼Œåˆå‘½å…³çœ‹ã€‚

    ç”«çŠ¹æŠ¥å…³æ›¹å†›å¸ˆæ­¦ä¿±å±±é”‹ï¼Œä¼‘è§ç„å¾·ã€‚åšåŠŸæ„ä¹…ï¼Œç«‹äº†åˆ˜å¤‡ï¼Œå˜‰ä¹ƒç½®è‡£è°‹é•¿è€³ã€‚
è€ç‘æ›°ï¼šâ€œâ€œå´æ‚Œå®é›„è¿ï¼Œä¼‘å¾—é€†æˆ‘ï¼Œéš¾æ¥ç”¨å°†ã€‚æ˜¨æè§é€šä¸ºè¿ã€‚å¾ä¸ä»¥å‘è®¡ä¹Ÿï¼Œå„æœ‰ä¸å¯ï¼Œåä¹‹ã€‚â€
ç‘œæƒŠæ›°ï¼šâ€œå­¤å¥‰æŸé©±åæ˜Ÿä¸å’Œï¼â€ä¸•å¤§æƒŠï¼Œä¹ƒä¸‹ä¸€é¢ä¸­è°ƒèµ–å±…ï¼Œå“¨å¾…æ›¹å…¬è€çˆ¶ï¼Œä¹ƒè¯¸å¿å‘ˆâ€œ
çƒ­è’å­å› å¤–è°­è‡ªï¼Œå¼•å†›äººéª‘æ³•ã€‚å›½ç§‹å¼•å…µè‡³æ¶ªåå¯¨ï¼Œå«è‚ƒåˆé€€ï¼Œå¨ä¸æƒ³é€èŠå¦‚å†³ï¼Œ
å¯ä½¿å°†è¿‡äº†ç„å¾·ã€‚å†›å£«ä¹‰è½¦ç»‘äº¤éƒ¡çœ‹ã€‚å½“æ—¥å±±æˆˆä»çŸ³æ±‰ä¸­å»äº†ï¼Œè¨€ç„å¾·å·²å¾—å®¶è‰ï¼Œ
å­™ç“’ä¾¿å­™ä¹¾å®ˆè€Œå¾ã€‚å¿½ç„¶ä¸€å¸œå››å±±è¯¸ä¸‡å°†å€¾å¦ï¼šâ€œä¸¤è°‹äººçš†è£¨å®é¥¿æ²³ï¼Œå››æµ·æœ‰è¿‡æ‰€åŠ›ï¼›
å·¦çœ‹åœ¨é‡‘ä¸­ï¼Œæ£å¾å·çš†ä¸Šç ´è¥ã€‚ç¥–é¦–æœå»·ä¸å—ï¼›åŠ¿å››è‚¡ç›Ÿï¼Œå¹¶è®¾çŒ–å³¡ï¼š

    å´è¯´æ¤é©±æ¥ï¼Œé¥±æ¥ä¹Ÿè§ï¼Œé“æ„Ÿæ¸©æ ¡ã€‚åªé—»å¾æ¿€é€è¨€åŸä¸ç„¶ã€‚

    ä¸”å†›é©¬æŠ¥å¾å·æ¸¡å¯¨ï¼Œå†æ”¶å°†å‚¬æŠ•â€œå°†åˆ†å¸ƒã€ä¹”æŒã€éŸ©ä»²ã€‚å„ã€å¼ ä»ªåˆ€æ‰‹äºå†›é©¬ã€‚
å°‘è´¥è¿›ï¼Œæ“é—®åºå¾·æ›°ï¼šâ€œæ±å¹³æœ‰çœŸå¦™çŸ£ï¼â€é˜¿è§†ä¹‹ï¼Œæ‹œä¾¿å•†è®®ã€‚ç“’éƒ¨å•†æƒ…ï¼šå·¦å³çœ‹é²ä¸ºå¤ªï¼Œ
è®¡å¿è¡¡æŒŸï¼Œæä¹ç•ä¸­ã€‚è¡¨æ–™ç¬¬æ°å—ä¹‹æƒ…ï¼Œæœªèƒœè€…æŠ¥å­”æ˜ï¼Œç”¨ç„¶å¤©ä¸‹ã€‚
å­”æ˜æ— åœ¨è½¦ä»—è‡³æ¡å…äº¡ï¼Œéšæ•™è½¬åˆ°å‘¨ä¸Šã€‚åäººæœ‰è¯—èµä¸€å¾’æ–¹è¯ºï¼Œæ¯—è¿›å…¥åºœåŸºã€‚
å…ˆè¯´å­”æ˜æ›°ï¼šâ€œæ€å¸åˆ˜ä¸šï¼Œä¸å¿å…­é”‹ä¹Ÿï¼â€å­”æ˜æ›°ï¼šâ€œæ±æ¥è¯´ä¸ç›¸å¤«æˆ˜ï¼Œäº¦ä½•é¼“é™ï¼Ÿâ€äºŒäººä¾¿åŒå…ˆé˜µå™¨ã€‚
```

ä½ æ˜¯å¦ä¹Ÿåƒæˆ‘ä¸€æ ·è®¤è®¤çœŸçœŸåœ°è¯»äº†ä¸€éï¼Ÿè™½ç„¶ä¸çŸ¥é“å®ƒåœ¨å†™ä»€ä¹ˆï¼Œä½† RNN å¯¹æ–‡è¨€æ–‡çš„æŒæ¡æˆ‘è¿˜æ˜¯è‡ªæ„§ä¸å¦‚ã€‚

### è€å‹è®°

ä¸€ä½ç½‘å‹åœ¨ Github ä¸Šç»´æŠ¤äº†è€å‹è®°å…¨ 10 å­£çš„å‰§æœ¬ï¼š[fangj/friends](https://github.com/fangj/friends)ï¼Œä»–åº”è¯¥ä¸ä¼šæƒ³åˆ°æœ‰ä¸€å¤©è¿™ä¸ªä»“åº“ä¼šå˜æˆ AI çš„ã€Œé¥²æ–™ã€ï¼Ÿå†™äº†ä¸€ä¸ªè„šæœ¬ç®€å•å°† html å¤„ç†æˆäº†æ™®é€šæ–‡æœ¬ (å»æ‰æ ‡ç­¾ï¼Œunescape ä¸€äº›ç‰¹æ®Šå­—ç¬¦)ï¼Œé€è¿›æ¨¡å‹è®­ç»ƒã€‚ä¸‹é¢æ˜¯å¾—åˆ°çš„å‰§æœ¬èŠ‚é€‰ï¼š

```
[Scene: Chandler and Joey's, Phoebe is answering from some gateen, the same assarent duck is
smile.]


Ross: Seriously, I canâ€™tâ€¦I play larny?


All: Oh, guys. This, this is where itâ€™s such some Agenton on the missay I could take
the bean in your side to me.

Phoebe: What? What do you think heâ€™s gonna see you?

Rachel: Really?! The only bay idea, but thatâ€™s okay! You gotta get rome,
tell me!

Rachel: Wow! What are you laughing after anything women?

Ross: Monnica told me about! This is beautiful that turts familiar. So I get a
button, fine. What are you doing?

Ross: Da me. (Pretends in door) I decided to cold use that.

Rachel: Whoa, whoa whoa, day! What a duving dollars!!!!
```

å¯ä»¥çœ‹å‡ºï¼ŒRNN æŒæ¡äº†å‰§æœ¬çš„ç»“æ„ï¼šåœºæ™¯ã€äººç‰©ã€å¯¹è¯ã€åŠ¨ä½œã€‚è™½ç„¶å¯¹è¯çœ‹èµ·æ¥æœ‰äº›ç©ºæ´ï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯å®ƒå­¦ä¼šäº† Rachel çš„è¯­æ°”ï¼š

```
Rachel: Whoa, whoa whoa, day! What a duving dollars!!!!
```

ä»ã€Œ `a` å’Œ `dollars` åŒæ—¶å‡ºç°ã€å¯ä»¥äº†è§£åˆ°å®ƒçš„è¯­æ³•è¿˜æœªå­¦åˆ°ä½ï¼Œä¸è¿‡å†™æˆè¿™æ ·è¿˜ä¸å€¼å¾—æˆ‘ä»¬é¼“åŠ±ä¸€ä¸‹ï¼Ÿ

### Kubernetes

[Codepilot](https://copilot.github.com/) å’Œ [AlphaCode](https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode) å·²ç»å‘å·¥ç¨‹å¸ˆä»¬è¯æ˜äº† AI å†™ä»£ç çš„èƒ½åŠ›ï¼Œæˆ‘ä¹Ÿæƒ³çœ‹çœ‹ 3 å±‚ LSTM èƒ½åšåˆ°ä»€ä¹ˆæ ·çš„ç¨‹åº¦ã€‚äºæ˜¯æˆ‘ clone äº† Kubernetes ä»“åº“ï¼Œå°†æ‰€æœ‰ Golang æºç èšåˆ (cat) æˆä¸€ä¸ªæ–‡æœ¬ (ä½ çŒœçŒœ Kubernetes çš„ codebase æ‰€å çš„ç©ºé—´æœ‰å¤šå¤§ï¼Ÿè¯„è®ºåŒºå‘Šè¯‰ä½ )ï¼Œé€è¿› torch-rnnï¼Œè®­ç»ƒä¸¤ä¸ªå°æ—¶ï¼Œé‡‡æ ·å‡ºäº†ä¸‹é¢è¿™æ®µä»£ç ï¼š

```go
// Reporter kubeconfig bootstrap implementation back of ne non-best or proto
// callered present to a new iterator that are build to
// seriily region.
type Instances struct {
	Interface framework.ClientVolumeInterface
	wait MustInfoDefaultCreatePrivilegedValue
	HostDNS    interface{}
	// Needed inshelt is data from bucket for checks, if the double with
	// this address group and needs used to less 4anology and https://github.com/policy/existing-storage/using/recevable-src image/shtge efility and intendaroping
	// store
	decisionKey *string

	// Prefer operations that successfully at I registry. See methods
	// instance (documentations) the console set4 are used.
	Codes bool
}

// NewPathParameter address taints of Server automatically.
func (pl *NormalGanglIngress) GetAll(data interface{}, contains *stderr.IsState) (bool, error) {
	key := &zs.FreeCheck{}
	_, err := d.info.CommandStatus(ctx)
	if err != nil {
		return err
	}
	error := field.ErrorTypes()
	return withRegisterBackoff(indexing)
}

// JSONNM currently directory was returned in it deleting the recorder and certificates.
type Attacher struct {
	obj uint32
	cidr []*Decls.ParseMessage
	sgip          io.SharedInformer
	Snapshot       *Token
}

func (s *symconfig) DescribeRestore(ctx context.Context, m file.Info) (bool, error) {
	return nil
}
```

ä»æœ€åä¸€ä¸ªå‡½æ•° `DescribeRestore` å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜æ²¡å‘ç°è¿”å›å‚æ•°çš„æ•°é‡éœ€è¦ä¸å‡½æ•°å®šä¹‰ä¿æŒä¸€è‡´ã€‚ä½†è‡³å°‘è¿™äº›ä»£ç è¢« Golang çš„è¯­æ³•é«˜äº®æ’ä»¶è®¤å¯äº†ï¼ç”šè‡³å®ƒè¿˜å­¦ä¼šäº†å†™æ³¨é‡Šã€‚åŒæ ·æœ‰æ„æ€çš„æ˜¯ï¼Œæˆ‘åœ¨å¹³æ—¶å·¥ä½œä¸­å†™ä»£ç æ—¶ï¼Œå¸¸å¸¸éœ€è¦ä¾èµ– `gofmt` æˆ– `goimports` è¿™æ ·çš„å·¥å…·è‡ªåŠ¨æ ¼å¼åŒ–ä»£ç ï¼Œæ¯”å¦‚ï¼š

```go
type Attacher struct {
	obj uint32
	cidr []*Decls.ParseMessage
	sgip          io.SharedInformer
	Snapshot       *Token
}
```

ç»è¿‡æ ¼å¼åŒ–åï¼Œä¼šå˜æˆï¼š

```go
type Attacher struct {
	obj      uint32
	cidr     []*Decls.ParseMessage
	sgip     io.SharedInformer
	Snapshot *Token
}
```

ä»ä¸Šé¢çš„ä»£ç æ¥çœ‹ï¼Œæˆ‘ä»¬çš„ RNN ä¼¼ä¹ä¹Ÿéœ€è¦è¿™æ ·çš„å·¥å…·ã€‚

### è¶…çº§ä¸¹çš„æŠ€æˆ˜æœ¯

è¿™æ˜¯ä¸€ä¸ªæˆ‘å°šæœªæœ‰ç²¾åŠ›å»å®Œæˆçš„å®éªŒï¼Œå› ä¸ºå®ƒçš„å‰æœŸæ•°æ®å¤„ç†å·¥ä½œé‡å¤ªå¤§ï¼Œåœ¨è¿™é‡Œæˆ‘ä»…ä»‹ç»ä¸€ä¸‹æƒ³æ³•ï¼š

å–œæ¬¢çœ‹/æ‰“ç¾½æ¯›çƒçš„æœ‹å‹éƒ½çŸ¥é“ï¼Œçƒå‘˜åœ¨åœºä¸Šæ­¥ä¼ (å¹¶æ­¥ã€è·‘æ­¥ã€äº¤å‰æ­¥ç­‰)ã€å‡»çƒæ–¹å¼ (é«˜ã€åŠã€æ€ã€æ“ã€æ”¾ç­‰)ã€çƒçš„è½ç‚¹ (å‰ã€ä¸­ã€åã€å·¦ã€ä¸­ã€å³ 9 ä¸ªç‚¹) çš„ä¸€ç³»åˆ—é€‰æ‹©ï¼Œæ„æˆçƒå‘˜çš„æŠ€æˆ˜æœ¯æ‰“æ³•ã€‚è¿™äº›é€‰æ‹©æ¥è‡ªäºçƒå‘˜å¯¹åœºä¸Šå±€åŠ¿çš„åˆ¤æ–­ (ä¹‹å‰çš„æ¥å›ã€å¯¹æ–¹çƒå‘˜çš„é€‰æ‹©ä»¥åŠåŒæ–¹çš„èº«å¿ƒçŠ¶æ€)ã€‚

å¦‚æœæˆ‘ä»¬èƒ½å°†è¿™äº›ä¿¡æ¯ç¼–ç æˆç¦»æ•£çš„çŠ¶æ€ï¼Œå°±èƒ½å°†ç¾½æ¯›çƒæ¯”èµ›æŠ½è±¡æˆæ—¶åºæ•°æ®ã€‚è¦å­¦å°±è¦ä»æœ€å¥½çš„å­¦ï¼Œæ‰€ä»¥æˆ‘çš„æƒ³æ³•æ˜¯ï¼šè·å–æ—ä¸¹å·…å³°æ—¶æœŸçš„æ‰€æœ‰æ¯”èµ›è§†é¢‘ï¼Œè®°å½•ä¸‹ä»–å’Œæ‰€æœ‰å¯¹æ‰‹åœ¨æ¯ä¸ªå›åˆåšå‡ºçš„çƒè·¯é€‰æ‹©ï¼Œç„¶åé€è¿› RNNã€‚æ˜¯ä¸æ˜¯æœ‰åŠ©äºæ–°çš„è¿åŠ¨å‘˜å¿«é€ŸæŒæ¡ã€Œè¶…çº§ä¸¹ã€çš„çƒè·¯ï¼Ÿå¸Œæœ›æˆ‘æœ‰ä¸€å¤©èƒ½æœ‰é—²æš‡æ—¶é—´æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚å¦‚æœçœ‹åˆ°è¿™ç¯‡åšå®¢çš„ä½ æ˜¯å›½å®¶ç¾½æ¯›çƒè®­ç»ƒä¸­å¿ƒçš„å·¥ä½œäººå‘˜ï¼Œæ¬¢è¿è”ç³»æˆ‘ï¼Œ just kidding  : )ã€‚

## å‚è€ƒèµ„æ–™

* [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Github: karpathy](https://github.com/karpathy), [char-rnn](https://github.com/karpathy/char-rnn), [min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086)
* [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)
* [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)
* [UFLDL Tutorial](http://ufldl.stanford.edu/tutorial/), [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
* [eliben/deep-learning-samples/min-char-rnn](https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py)
*  [nicodjimenez/lstm](https://github.com/nicodjimenez/lstm/)
* [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [ZhengHe-MD/replay-nn-tutorials/min-char-rnn](https://github.com/ZhengHe-MD/replay-nn-tutorials/tree/main/min-char-rnn)
