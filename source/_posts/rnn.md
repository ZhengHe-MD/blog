---
title: ä»å¤´å¼€å§‹å®ç° RNN
date: 2022-02-20 15:03:20
tags:
---

> What I cannot create, I do not understand. -- Richard Feynman

Andrej Karpathy åœ¨ 2015 å¹´å‘è¡¨äº†é¢˜ä¸º [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) çš„åšå®¢ï¼Œå¹¶é…å¥—å¼€æºå…¶ä¸­å®éªŒæ‰€ç”¨çš„[char-rnn ä»£ç ä»“åº“](https://github.com/karpathy/char-rnn)ï¼Œä»¥åŠç”¨ numpy æ‰‹å†™çš„ [gist: min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086)ï¼Œé˜…è¯»è¿‡åå—ç›Šè‰¯å¤šã€‚äºæ˜¯è¿™ä¸¤å¤©èŠ±äº†äº›æ—¶é—´ï¼š

1. é€è¡Œç†è§£ min-char-rnn
2. åŸºäºç†è§£å’ŒåŸè„šæœ¬å®ç°äº† 2-layer å’Œ n-layer çš„ RNN

æ•´ä¸ªæ¢ç´¢è¿‡ç¨‹å……æ»¡äº†è¶£å‘³å’ŒæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºä¸€ä½ä¸»è¥æœåŠ¡ç«¯å¼€å‘çš„å·¥ç¨‹å¸ˆï¼Œå› æ­¤ç‰¹æ„å°†è¿™ä¸ªè¿‡ç¨‹è®°å½•ä¸‹æ¥ã€‚

<!-- more -->

> âš ï¸ æœ¬æ–‡å‡è®¾è¯»è€…æœ‰ä¸€å®šçš„æ·±åº¦å­¦ä¹ ç†è®ºåŸºç¡€å’Œå®è·µç»éªŒï¼ŒåŒæ—¶é˜…è¯»è¿‡ä¸Šè¿°çš„åšå®¢å’Œè„šæœ¬ã€‚ä½†å¦‚æœä¸ä»‹æ„å¯èƒ½å‡ºç°çš„ç†è§£éšœç¢ï¼Œä¹Ÿæ¬¢è¿ç»§ç»­é˜…è¯»æœ¬æ–‡ã€‚

## ç†è§£ min-char-rnn

min-char-rnn å®ç°çš„æ˜¯å•å±‚çš„ Vanilla RNNï¼Œå…¶æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of min-char-rnn](./one-layer.png)

> ğŸ’¡ ä¸€ä½çƒ­å¿ƒç½‘å‹å°±ç€ min-char-rnn ç»™å‡ºäº†ä¸€ä¸ªå¸¦æ³¨é‡Šçš„[ç‰ˆæœ¬](https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py)ï¼Œé˜…è¯»åè€…çš„éš¾åº¦ä¼šæ›´å°ä¸€äº›

### å‰å‘ä¼ æ’­

ä»¥ä¸‹æ˜¯æºç ä¸­çš„å‰å‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
for t in xrange(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
```

é€šå¸¸æ¯”è¾ƒå·§å¦™çš„åœ°æ–¹å°±æ˜¯ä»¤äººè´¹è§£çš„åœ°æ–¹ï¼Œè¿™é‡Œæ¯”è¾ƒå·§å¦™çš„è‡ªç„¶æ˜¯ CrossEntropy çš„è®¡ç®—ï¼š

```python
loss += -np.log(ps[t][targets[t],0])
```

CrossEntropy è®¡ç®—çš„æ˜¯ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ç¨‹åº¦ï¼Œå·®å¼‚è¶Šå¤§ï¼Œç†µå€¼è¶Šå¤§ã€‚è¿™é‡Œçš„ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒåˆ†åˆ«æ˜¯ã€ŒRNN é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€å’Œã€Œå®é™…ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€ï¼Œåœ¨ç›‘ç£å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œåè€…æ˜¯ä¸€ä»¶å·²ç»ç¡®å®šçš„äº‹æƒ…ï¼Œæ‰€ä»¥å®ƒçš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ [0, 0, ..., 1, 0, 0]ï¼Œå³é™¤ä¸€ä¸ªç¡®å®šå­—ç¬¦çš„å‡ºç°æ¦‚ç‡ä¸º 1 å¤–ï¼Œå‰©ä½™å­—ç¬¦çš„å‡ºç°æ¦‚ç‡çš†ä¸º 0ã€‚äºæ˜¯æœ€ç»ˆå¯¹æŸå¤±å‡½æ•°äº§ç”Ÿè´¡çŒ®çš„åªæœ‰ä¸€ä¸ªå­—ç¬¦å‡ºç°çš„æ¦‚ç‡ï¼Œå³è¿™é‡Œçš„ `ps[t][targets[t], 0]`ã€‚

### åå‘ä¼ æ’­

ä»¥ä¸‹æ˜¯æºç ä¸­çš„åå‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
for t in reversed(xrange(len(inputs))):
    dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
    dWhy += np.dot(dy, hs[t].T)
    dby += dy
    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)
```

å¦‚æœä½ åƒæˆ‘ä¸€æ ·åªå¯¹æ•°å­¦åˆ†æä¸­æœ€åŸºæœ¬çš„ã€Œå•å˜é‡æ±‚å¯¼ã€è¿˜ç•™å­˜æœ‰ä¸€å®šçš„è®°å¿†ï¼Œé‚£çœ‹è¿™æ®µä»£ç ä¼šæœ‰ç§ã€Œè¡¨é¢ä¸Šå¥½åƒèƒ½çœ‹æ‡‚ï¼Œä»”ç»†æ€è€ƒåˆè§‰å¾—å“ªé‡Œå¯¹ä¸ä¸Šã€çš„æ„Ÿè§‰ã€‚å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“çœ‹å‡ºæ¥è¿™äº›ä»£ç æ˜¯åœ¨å°†æŸå¤±å‡½æ•°çš„å˜åŒ–æŒ‰ç›¸åçš„æ–¹å‘ä¸€æ­¥ä¸€æ­¥åœ°å€’æ¨å›å»ï¼Œå†åŠ ä¸Šè„‘æµ·ä¸­å°šå­˜çš„å•å˜é‡ã€Œé“¾å¼æ±‚å¯¼ã€è§„åˆ™ï¼Œä¼¼ä¹ä¸€åˆ‡é¡ºç†æˆç« ã€‚ä½†ç»†æ€æã€Œæã€ï¼Œè¿™é‡Œé¢æœ‰æ ‡é‡ã€æœ‰å‘é‡ï¼Œè¿˜æœ‰çŸ©é˜µï¼Œä»€ä¹ˆæ˜¯ã€Œæ ‡é‡å¯¹çŸ©é˜µæ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€Œå‘é‡å¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€ŒçŸ©é˜µå¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€ŒçŸ©é˜µå¯¹çŸ©é˜µæ±‚å¯¼ã€ï¼Ÿæˆ‘ä»¬å­¦è¿‡ã€Œå‘é‡æ±‚å¯¼ã€æˆ–è€…ã€ŒçŸ©é˜µæ±‚å¯¼ã€å—ï¼Ÿ

ä¸ºäº†æ¶ˆè§£è„‘æµ·ä¸­å°šä¸æ¸…æ™°çš„åœ°æ–¹ï¼Œæˆ‘åœ¨æœç´¢å¼•æ“ä¸­ä»¥ç±»ä¼¼ã€Œmatrix calculus for engineerã€çš„å…³é”®è¯ï¼Œæ‰¾åˆ°äº†ä¸€ç¯‡é•¿æ–‡ï¼š[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)ï¼Œå®ƒæ­£å¥½æ˜¯ä¸ºåƒæˆ‘è¿™æ ·**åªè®°å¾—ã€Œå•å˜é‡æ±‚å¯¼ã€çš„äºº**é‡èº«å®šåšçš„èµ„æ–™ï¼Œäºæ˜¯æˆ‘é›†ä¸­ç²¾åŠ›èŠ±äº† 3-4 ä¸ªå°æ—¶æŠŠå…¨æ–‡é€šè¯»ï¼Œéšååœ¨ç¬”è®°æœ¬ä¸Šå°†ä»£ç ä¸­çš„å‡ ä¸ªæ ‡é‡ (æŸå¤±å‡½æ•°)ã€å‘é‡ã€çŸ©é˜µé—´çš„æ±‚å¯¼éƒ½æ¨äº†ä¸€éï¼Œç»ˆäºäº‘å¼€é›¾æ•£ï¼Œäº‹å®è¯æ˜è¿™æ ·çš„ä»˜å‡ºååˆ†å€¼å¾—ã€‚

Andrej åœ¨åå‘ä¼ æ’­ Softmax + CrossEntropy æ—¶å†™äº†ä¸ªå¤‡æ³¨ï¼š

> backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here

ä»¤äººæ²®ä¸§çš„æ˜¯ï¼Œå½“ä½ ä»”ç»†é˜…è¯»è¿™éƒ¨åˆ†å†…å®¹ï¼Œä¼šå‘ç°è¿™æ ·ä¸€å¥è¯ï¼š

> Itâ€™s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out...

çœŸæ­£çœ‹æ‡‚å®ƒï¼Œæ˜¯æˆ‘åœ¨é˜…è¯»å®Œä¸Šé¢é‚£ç¯‡å…³äº Matrix Calculus çš„æ‰«ç›²æ–‡ç« ï¼Œç´§æ¥ç€ç»§ç»­é˜…è¯»è¿™ç¯‡åšå®¢ [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) ä¹‹åã€‚**æœ€ç»ˆçš„ç­”æ¡ˆå¾ˆå…·æœ‰ç¾æ„Ÿï¼Œä½†æ¨å¯¼çš„è¿‡ç¨‹å¾ˆéœ€è¦è€å¿ƒ**ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™é‡Œåœ¨è®­ç»ƒçš„æ—¶å€™è¿˜ä½¿ç”¨äº†ã€Œteacher forcingã€çš„æŠ€å·§ï¼š

```python
dy = np.copy(ps[t])
dy[targets[t]] -= 1
```

å³ä¸ç®¡ RNN åœ¨æ¯æ¬¡ unroll éƒ½ä½¿ç”¨æ ‡å‡†ç­”æ¡ˆï¼Œè€Œä¸æ˜¯è‡ªèº«å­¦åˆ°çš„ç»“æœ `np.argmax(ps[t])`ï¼Œåœ¨å…¶å®ƒæ•™ç¨‹ä¸­ï¼Œæˆ‘ä¹Ÿçœ‹åˆ°è¿‡äººä»¬ä¼šç»™ã€Œteacher forcingã€æ–½åŠ ä¸€ä¸ªæ¦‚ç‡ï¼Œå°±åƒ dropout_pï¼Œå› ä¸ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ä¼šæœ‰ä»»ä½•å­—ç¬¦çš„å‚è€ƒç»“æœï¼Œæ®è¯´é€šè¿‡è¿™ä¸ªæ¦‚ç‡å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šé¿å…ã€Œå­¦ç”Ÿåœ¨å®é™…è§£é¢˜æ—¶è¿‡åˆ†ä¾èµ–è€å¸ˆã€ã€‚

### Gradient Check

åœ¨ min-char-rnn è„šæœ¬ä¸‹çš„ç¬¬ä¸€ä¸ªè¯„è®ºï¼Œå°±æ˜¯ Andrej è‡ªå·±çš„ï¼š

![gradient check](./gradient-check.png)

åˆšçœ‹åˆ°è¿™æ®µä»£ç ï¼Œæˆ‘å®Œå…¨ä¸çŸ¥é“ gradient check æ˜¯åœ¨åšä»€ä¹ˆï¼Œè‡ªç„¶æ— æ³•ç†è§£è¿™æ®µä»£ç çš„å«ä¹‰ã€‚ä¸è¿‡åœ¨æœç´¢å¼•æ“çš„å¸®åŠ©ä¸‹ï¼Œæ‰¾åˆ°äº†æ–¯å¦ç¦å¤§å­¦å¼€æ”¾çš„æ•™ç¨‹ [Unsupervised Feature Learning and Deep Learning](http://ufldl.stanford.edu/tutorial/)ï¼Œå…¶ä¸­ä¸€èŠ‚æ­£æ˜¯ä»‹ç» [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)ã€‚æ‰€è°“ gradient check(ing) å…¶å®å°±æ˜¯å¯¹æ¯”ã€Œé€šè¿‡åå‘ä¼ æ’­è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦ã€ä¸ã€Œåˆ©ç”¨å¯¼æ•°å®šä¹‰è¿‘ä¼¼è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ã€ï¼Œæ¥åˆ¤æ–­è‡ªå·±çš„åå‘ä¼ æ’­é˜¶æ®µä»£ç æ˜¯å¦å†™å¯¹äº†ï¼Œæ˜¯å·¥ç¨‹å¸ˆæ„å»ºç¥ç»ç½‘ç»œæ—¶å¸¸ç”¨çš„ä¸€ç§ç®€å•æœ‰æ•ˆçš„ debug æ–¹æ³•ã€‚ç”±äºæ¯æ¬¡è®­ç»ƒéƒ½å¯èƒ½éå¸¸è€—æ—¶ï¼Œè€Œä¸”ä¸€äº› bug å³ä¾¿å­˜åœ¨ï¼Œè¡¨é¢ä¸Šä¹Ÿå¯èƒ½çœ‹ç€ä¸€åˆ‡æ­£å¸¸ï¼Œåœ¨æ„å»ºå®Œç¥ç»ç½‘ç»œåï¼Œå¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæ‰§è¡Œä¸€ä¸‹ gradient check(ing) å¾ˆæœ‰å¿…è¦ã€‚åœ¨å®ç° 2-layer å’Œ n-layer RNN çš„è¿‡ç¨‹ä¸­ï¼Œgradient check(ing) å°±æˆåŠŸå¸®åŠ©æˆ‘å‘ç°äº†ä»£ç ä¸­çš„è‹¥å¹²é€»è¾‘é—®é¢˜ï¼Œè¿™æ ·çš„é€»è¾‘é—®é¢˜é€šè¿‡ä¼ ç»Ÿçš„ã€Œprint è°ƒè¯•ã€ã€ã€Œå•ç‚¹è°ƒè¯•ã€ã€ã€Œçœ¼ç¥è°ƒè¯•ã€éƒ½æéš¾å‘ç°ã€‚

## å®ç° 2-layer å’Œ n-layer RNN

2-layer çš„ RNN ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of a 2-layer RNN](./two-layer.png)

æºç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_two_layers.py)æ‰¾åˆ°ï¼Œå…¶ä¸­çš„å˜é‡å‘½åä¸ä¸Šå›¾çš„ç»“æ„ä¸€è‡´ã€‚

n-layer çš„ RNN ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of a n-layer RNN](./n-layer.png)

æºç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_n_layers.py)æ‰¾åˆ°ï¼Œå…¶ä¸­çš„å˜é‡å‘½åä¸ä¸Šå›¾çš„ç»“æ„ä¸€è‡´ã€‚

## å°¾å£°

åç»­æˆ‘å°†ç»§ç»­åœ¨å·¥ä½œä¹‹ä½™ï¼Œå°è¯•è£¸å†™ LSTM å’Œ GRUï¼Œç„¶åé€æ­¥å¤ç° Andrej åœ¨ä¸ƒå¹´å‰å®Œæˆçš„å…¶å®ƒå®éªŒ : )ã€‚

## å‚è€ƒèµ„æ–™

* [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Github: karpathy](https://github.com/karpathy), [char-rnn](https://github.com/karpathy/char-rnn), [min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086)
* [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)
* [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)
* [UFLDL Tutorial](http://ufldl.stanford.edu/tutorial/), [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
* [eliben/deep-learning-samples/min-char-rnn](https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py)
* [ZhengHe-MD/replay-nn-tutorials/min-char-rnn](https://github.com/ZhengHe-MD/replay-nn-tutorials/tree/main/min-char-rnn)
