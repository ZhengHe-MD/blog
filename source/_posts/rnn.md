---
title: ä»å¤´å¼€å§‹å®ç° RNN (ing)
date: 2022-02-20 15:03:20
tags:
---

> What I cannot create, I do not understand. -- Richard Feynman

Andrej Karpathy åœ¨ 2015 å¹´å‘è¡¨äº†é¢˜ä¸º [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) çš„åšå®¢ï¼Œå¹¶é…å¥—å¼€æºäº†å…¶ä¸­å®éªŒæ‰€ç”¨çš„[char-rnn ä»£ç ä»“åº“](https://github.com/karpathy/char-rnn)ï¼Œä»¥åŠç”¨ numpy æ‰‹å†™çš„ [gist: min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086)ï¼Œé˜…è¯»è¿‡åå—ç›Šè‰¯å¤šã€‚äºæ˜¯æœ€è¿‘èŠ±äº†ä¸€äº›æ—¶é—´åšäº†ä¸‹é¢è¿™äº›äº‹æƒ…ï¼š

1. é€è¡Œç†è§£ min-char-rnnï¼Œå³ vanilla RNN

2. å®ç° N å±‚ vanilla RNN

3. å®ç° LSTM (Long Short-Term Memory) RNN

4. æ¢ç´¢ RNN çš„å¯èƒ½æ€§

   * æ•£æ–‡ç”Ÿæˆå™¨ (Paul Graham å’Œä¸œå‘æ°´å“¥)
   * ç”Ÿæˆ TiDB å’Œ K8s ä»£ç 

   * æ—ä¸¹çš„æŠ€æˆ˜æœ¯å‰–æ
   * ... (å¾…è¡¥å……)

æ•´ä¸ªæ¢ç´¢è¿‡ç¨‹å¯¹æˆ‘è€Œè¨€å……æ»¡äº†è¶£å‘³å’ŒæŒ‘æˆ˜ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­ä»¤äººæ¿€åŠ¨åœ°é¦–æ¬¡ä½¿ç”¨å¾®ç§¯åˆ†çš„çŸ¥è¯†ã€‚å°½ç®¡è¿™åªæ˜¯æ·±åº¦å­¦ä¹ çš„å†°å±±ä¸€è§’ï¼Œä½†è¶³ä»¥è®©ä¸€åä¸»è¥ä¸šåŠ¡ä¸ºæœåŠ¡ç«¯å¼€å‘çš„è½¯ä»¶å·¥ç¨‹å¸ˆæ„Ÿåˆ°æ¿€åŠ¨ä¸å·²ï¼Œäºæ˜¯ä¾¿æœ‰äº†è¿™ç¯‡åšå®¢ï¼Œå°†è¿™ä¸ªè¿‡ç¨‹è®°å½•ä¸‹æ¥ã€‚

<!-- more -->

åœ¨æ„æ€è¿™ç¯‡åšå®¢å‰ï¼Œæˆ‘æ›¾ç»æƒ³è¿‡å†™ä¸€ä¸ªå®Œæ•´çš„ã€ç»†è‡´å…¥å¾®çš„ã€ä¿å§†å¼çš„ä» 0 - 1 å®ç° RNN çš„æ•™ç¨‹ã€‚åæ¥å‘ç°ï¼Œåœ¨æˆ‘è‡ªå·±é˜…è¯»ã€ç†è§£å’Œå®ç°çš„è¿‡ç¨‹ä¸­ï¼Œè‡³å°‘æŠ•å…¥æ•°åä¸ªå°æ—¶é˜…è¯»å¤§é‡èµ„æ–™ï¼ŒæœŸé—´å¹¶æ²¡æœ‰å‘ç°ä»»ä½•ä¸€ç¯‡æ–‡ç« èƒ½åšåˆ°è¿™ç‚¹ï¼Œé‚£ä¹ˆæˆ‘åˆå¦‚ä½•èƒ½æœŸæœ›å†™ä¸€ç¯‡åšå®¢åšåˆ°è¿™ç‚¹ï¼Ÿ

![standards](https://imgs.xkcd.com/comics/standards.png)

é‚£ä»€ä¹ˆä¸œè¥¿æ˜¯å€¼å¾—åˆ†äº«çš„å‘¢ï¼Ÿæˆ‘æ€è€ƒçš„ç»“æœæ˜¯ã€Œå­¦ä¹ è·¯å¾„ã€ã€‚åœ¨é‡åˆ°ç†è§£éšœç¢æ—¶ï¼Œæˆ‘å°è¯•å¯»æ‰¾å¹¶é˜…è¯»äº†å“ªäº›èµ„æ–™ï¼ŒèŠ±è´¹äº†å¤šé•¿æ—¶é—´ï¼Œæœ‰ä»€ä¹ˆå¿ƒå¾—ä½“ä¼šï¼Œè¿™äº›ä¹Ÿè®¸èƒ½å¤Ÿç»™åˆ°è¯»è€…çµæ„Ÿã€‚

> âš ï¸ æœ¬æ–‡å‡è®¾è¯»è€…æœ‰ä¸€å®šçš„æ·±åº¦å­¦ä¹ ç†è®ºåŸºç¡€å’Œå®è·µç»éªŒï¼ŒåŒæ—¶é˜…è¯»è¿‡ä¸Šè¿°çš„åšå®¢å’Œè„šæœ¬ã€‚ä½†å¦‚æœä¸ä»‹æ„å¯èƒ½å‡ºç°çš„ç†è§£éšœç¢ï¼Œä¹Ÿæ¬¢è¿ç»§ç»­é˜…è¯»æœ¬æ–‡ã€‚
>
> ğŸ’¡ ä¸€ä½çƒ­å¿ƒç½‘å‹å°±ç€ min-char-rnn ç»™å‡ºäº†ä¸€ä¸ªå¸¦æ³¨é‡Šçš„[ç‰ˆæœ¬](https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py)ï¼Œé˜…è¯»åè€…çš„éš¾åº¦ä¼šæ›´å°ä¸€äº›

## ç†è§£ min-char-rnn

min-char-rnn å®ç°çš„æ˜¯å•å±‚çš„ vanilla RNNï¼Œå…¶æ•´ä½“ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of min-char-rnn](./one-layer.png)

### å‰å‘ä¼ æ’­

ä»¥ä¸‹æ˜¯æºç ä¸­çš„å‰å‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
for t in xrange(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
```

é€šå¸¸æ¯”è¾ƒç²¾å·§çš„åœ°æ–¹å°±æ˜¯ä»¤äººè´¹è§£çš„åœ°æ–¹ï¼Œè¿™é‡Œæ¯”è¾ƒç²¾å·§çš„è‡ªç„¶æ˜¯ CrossEntropy çš„è®¡ç®—ï¼š

```python
loss += -np.log(ps[t][targets[t],0])
```

CrossEntropy è®¡ç®—çš„æ˜¯ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ç¨‹åº¦ï¼Œå·®å¼‚è¶Šå¤§ï¼Œç†µå€¼è¶Šå¤§ã€‚è¿™é‡Œçš„ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒåˆ†åˆ«æ˜¯ã€ŒRNN é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€å’Œã€Œå®é™…ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€ï¼Œåœ¨ç›‘ç£å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œåè€…æ˜¯ä¸€ä»¶å·²ç»ç¡®å®šçš„äº‹æƒ…ï¼Œæ‰€ä»¥å®ƒçš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ [0, 0, ..., 1, 0, 0]ï¼Œå³é™¤ä¸€ä¸ªç¡®å®šå­—ç¬¦çš„å‡ºç°æ¦‚ç‡ä¸º 1 å¤–ï¼Œå‰©ä½™å­—ç¬¦çš„å‡ºç°æ¦‚ç‡çš†ä¸º 0ã€‚äºæ˜¯æœ€ç»ˆå¯¹æŸå¤±å‡½æ•°äº§ç”Ÿè´¡çŒ®çš„åªæœ‰æŸä¸€ä¸ªå­—ç¬¦å‡ºç°çš„æ¦‚ç‡ï¼Œå³è¿™é‡Œçš„ `ps[t][targets[t], 0]`ã€‚

### åå‘ä¼ æ’­

ä»¥ä¸‹æ˜¯æºç ä¸­çš„åå‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
for t in reversed(xrange(len(inputs))):
    dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
    dWhy += np.dot(dy, hs[t].T)
    dby += dy
    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)
```

å¦‚æœä½ åƒæˆ‘ä¸€æ ·åªå¯¹æ•°å­¦åˆ†æä¸­æœ€åŸºæœ¬çš„ã€Œå•å˜é‡æ±‚å¯¼ã€è¿˜ç•™å­˜æœ‰ä¸€å®šçš„è®°å¿†ï¼Œé‚£çœ‹è¿™æ®µä»£ç ä¼šæœ‰ç§ã€Œè¡¨é¢ä¸Šå¥½åƒèƒ½çœ‹æ‡‚ï¼Œä»”ç»†æ€è€ƒåˆè§‰å¾—å“ªé‡Œå¯¹ä¸ä¸Šã€çš„æ„Ÿè§‰ã€‚å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“çœ‹å‡ºæ¥è¿™äº›ä»£ç æ˜¯åœ¨å°†æŸå¤±å‡½æ•°çš„å˜åŒ–æŒ‰ç›¸åçš„æ–¹å‘ä¸€æ­¥ä¸€æ­¥åœ°å€’æ¨å›å»ï¼Œå†åŠ ä¸Šè„‘æµ·ä¸­å°šå­˜çš„å•å˜é‡ã€Œé“¾å¼æ±‚å¯¼ã€è§„åˆ™ï¼Œä¼¼ä¹ä¸€åˆ‡é¡ºç†æˆç« ã€‚ä½†ã€Œç»†æ€ææã€ï¼Œè¿™é‡Œé¢æœ‰æ ‡é‡ã€æœ‰å‘é‡ï¼Œè¿˜æœ‰çŸ©é˜µï¼Œä»€ä¹ˆæ˜¯ã€Œæ ‡é‡å¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€Œæ ‡é‡å¯¹çŸ©é˜µæ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€Œå‘é‡å¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€ŒçŸ©é˜µå¯¹å‘é‡æ±‚å¯¼ã€ï¼Ÿä»€ä¹ˆæ˜¯ã€ŒçŸ©é˜µå¯¹çŸ©é˜µæ±‚å¯¼ã€ï¼Ÿæˆ‘ä»¬å­¦è¿‡ã€Œå‘é‡æ±‚å¯¼ã€æˆ–è€…ã€ŒçŸ©é˜µæ±‚å¯¼ã€å—ï¼Ÿ

ä¸ºäº†æ¶ˆè§£è„‘æµ·ä¸­å°šä¸æ¸…æ™°çš„åœ°æ–¹ï¼Œæˆ‘é€šè¿‡æœç´¢å¼•æ“ï¼Œä»¥ç±»ä¼¼ã€Œmatrix calculus for engineerã€çš„å…³é”®è¯ï¼Œæ‰¾åˆ°äº†ä¸€ç¯‡é•¿æ–‡ï¼š[The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)ï¼Œå®ƒæ­£å¥½æ˜¯ä¸ºåƒæˆ‘è¿™æ ·**åªè®°å¾—ã€Œå•å˜é‡æ±‚å¯¼ã€çš„äºº**é‡èº«å®šåšçš„èµ„æ–™ã€‚äºæ˜¯æˆ‘é›†ä¸­ç²¾åŠ›èŠ±äº† 3-4 ä¸ªå°æ—¶æŠŠå…¨æ–‡é€šè¯»ï¼Œéšååœ¨ç¬”è®°æœ¬ä¸Šå°†ä»£ç ä¸­çš„å‡ ä¸ªæ ‡é‡ (æŸå¤±å‡½æ•°)ã€å‘é‡ã€çŸ©é˜µé—´çš„æ±‚å¯¼éƒ½æ¨äº†ä¸€éï¼Œç»ˆäºäº‘å¼€é›¾æ•£ï¼Œäº‹å®è¯æ˜è¿™æ ·çš„ä»˜å‡ºååˆ†å€¼å¾—ã€‚

Andrej åœ¨åå‘ä¼ æ’­ Softmax + CrossEntropy æ—¶å†™äº†ä¸ªå¤‡æ³¨ï¼š

> backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here

ä»¤äººæ²®ä¸§çš„æ˜¯ï¼Œå½“ä½ ä»”ç»†é˜…è¯»è¿™éƒ¨åˆ†å†…å®¹ï¼Œä¼šå‘ç°è¿™æ ·ä¸€å¥è¯ï¼š

> Itâ€™s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out...

çœŸæ­£çœ‹æ‡‚å®ƒï¼Œæ˜¯æˆ‘åœ¨é˜…è¯»å®Œä¸Šé¢é‚£ç¯‡å…³äº Matrix Calculus çš„æ‰«ç›²æ–‡ç« ï¼Œç´§æ¥ç€ç»§ç»­é˜…è¯»è¿™ç¯‡åšå®¢ [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) ä¹‹åã€‚**æœ€ç»ˆçš„ç­”æ¡ˆå¾ˆå…·æœ‰ç¾æ„Ÿï¼Œä½†æ¨å¯¼çš„è¿‡ç¨‹å¾ˆéœ€è¦è€å¿ƒ**ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™é‡Œåœ¨è®­ç»ƒçš„æ—¶å€™è¿˜ä½¿ç”¨äº†ã€Œteacher forcingã€çš„æŠ€å·§ï¼š

```python
dy = np.copy(ps[t])
dy[targets[t]] -= 1
```

å³ä¸ç®¡ RNN åœ¨æ¯æ¬¡ unroll éƒ½ä½¿ç”¨æ ‡å‡†ç­”æ¡ˆï¼Œè€Œä¸æ˜¯è‡ªèº«å­¦åˆ°çš„ç»“æœ `np.argmax(ps[t])`ã€‚åœ¨å…¶å®ƒæ•™ç¨‹ä¸­ï¼Œæˆ‘ä¹Ÿçœ‹åˆ°è¿‡äººä»¬ä¼šç»™ã€Œteacher forcingã€æ–½åŠ ä¸€ä¸ªæ¦‚ç‡ï¼Œå°±åƒ `dropout_p`ï¼Œå› ä¸ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ä¼šæœ‰ä»»ä½•å­—ç¬¦çš„å‚è€ƒç»“æœã€‚æ®è¯´é€šè¿‡é™ä½è¿™ä¸ªæ¦‚ç‡å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šé¿å…ã€Œå­¦ç”Ÿåœ¨å®é™…è§£é¢˜æ—¶è¿‡åˆ†ä¾èµ–è€å¸ˆã€ã€‚

### Gradient check

åœ¨ min-char-rnn è„šæœ¬ä¸‹çš„ç¬¬ä¸€ä¸ªè¯„è®ºï¼Œå°±æ˜¯ Andrej è‡ªå·±çš„ï¼š

![gradient check](./gradient-check.png)

åˆšçœ‹åˆ°è¿™æ®µä»£ç ï¼Œæˆ‘å®Œå…¨ä¸çŸ¥é“ gradient check æ˜¯åœ¨åšä»€ä¹ˆï¼Œè‡ªç„¶ä¹Ÿæ— æ³•ç†è§£è¿™æ®µä»£ç çš„å«ä¹‰ã€‚ä¸è¿‡åœ¨æœç´¢å¼•æ“çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘è¿˜æ˜¯æ‰¾åˆ°äº†æ–¯å¦ç¦å¤§å­¦å¼€æ”¾çš„æ•™ç¨‹ [Unsupervised Feature Learning and Deep Learning](http://ufldl.stanford.edu/tutorial/)ï¼Œå…¶ä¸­ä¸€èŠ‚æ­£æ˜¯ä»‹ç» [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)ã€‚æ‰€è°“ gradient check(ing) å…¶å®å°±æ˜¯å¯¹æ¯”ã€Œé€šè¿‡åå‘ä¼ æ’­è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦ã€ä¸ã€Œåˆ©ç”¨å¯¼æ•°å®šä¹‰è¿‘ä¼¼è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ã€ï¼Œæ¥åˆ¤æ–­è‡ªå·±çš„åå‘ä¼ æ’­é˜¶æ®µä»£ç æ˜¯å¦å†™å¯¹äº†ï¼Œæ˜¯å·¥ç¨‹å¸ˆæ„å»ºç¥ç»ç½‘ç»œæ—¶å¸¸ç”¨çš„ä¸€ç§ç®€å•æœ‰æ•ˆçš„ debug æ–¹æ³•ã€‚ç”±äºæ¯æ¬¡è®­ç»ƒéƒ½å¯èƒ½éå¸¸è€—æ—¶ï¼Œè€Œä¸”ä¸€äº›é—®é¢˜å³ä¾¿å­˜åœ¨ï¼Œè¡¨é¢ä¸Šä¹Ÿå¯èƒ½çœ‹ç€ä¸€åˆ‡æ­£å¸¸ã€‚åœ¨æ„å»ºå®Œç¥ç»ç½‘ç»œåï¼Œå¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæ‰§è¡Œ gradient check(ing) å¾ˆæœ‰å¿…è¦ã€‚

æˆ‘åœ¨å®ç° 2-layer å’Œ n-layer RNN çš„è¿‡ç¨‹ä¸­ï¼Œå°±æˆåŠŸåˆ©ç”¨ gradient check(ing) å‘ç°ä»£ç ä¸­çš„è‹¥å¹²é€»è¾‘é—®é¢˜ï¼Œè¿™æ ·çš„é€»è¾‘é—®é¢˜é€šè¿‡ä¼ ç»Ÿçš„ã€Œçœ¼ç¥è°ƒè¯•ã€ã€ã€Œprint è°ƒè¯•ã€ã€ã€Œå•ç‚¹è°ƒè¯•ã€éƒ½æéš¾å‘ç°ã€‚

## N å±‚ vanilla RNN

åœ¨é˜…è¯» min-char-rnn æ—¶ï¼Œæˆ‘åœ¨å¿ƒé‡Œå°±èŒç”Ÿäº†ä¸€ä¸ªæƒ³æ³•ï¼šèƒ½å¦ä¾è‘«èŠ¦ç”»ç“¢ç›´æ¥ç”¨ numpy å®ç°ä¸€ä¸ª N å±‚ vanilla RNNï¼Ÿä½†æ­¥å­å¤ªå¤§å®¹æ˜“æ‰¯ç€è›‹ï¼Œå…ˆæŒ‘æˆ˜ä¸€ä¸ª 2 å±‚çš„ vanilla RNNï¼š

![The architecture of a 2-layer RNN](./two-layer.png)

åœ¨ç»“æ„ä¸Šï¼Œ2 å±‚çš„ vanilla RNN å¤šäº†ä¸€ä¸ª hidden layerï¼Œç¬¬äºŒå±‚ (h2) çš„è¾“å…¥å°±æ˜¯ç¬¬ä¸€å±‚ (h1) çš„è¾“å‡ºï¼Œä¸»è¦åŒºåˆ«åœ¨äºä¸¤å±‚çš„è¾“å…¥ç»´åº¦ä¸åŒï¼Œå› ä¸ºè¿™ç‚¹ä¸åŒï¼Œåœ¨ä»£ç ä¸­éœ€è¦åˆ†å¼€å®ç°å®ƒä»¬çš„ä¼ æ’­é€»è¾‘ã€‚æˆ‘çš„å®ç°å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_two_layers.py)æ‰¾åˆ°ï¼Œä»£ç ä¸­çš„å˜é‡å‘½åä¸å›¾ä¸­çš„æ ‡è¯†ä¿æŒä¸€è‡´ã€‚å®ç°åŸºæœ¬é€»è¾‘å¤§çº¦èŠ±è´¹äº†åŠä¸ªå°æ—¶æ—¶é—´ï¼Œä½†ç”±äºå¯¹ numpy çš„ä¸ç†Ÿæ‚‰ä»¥åŠä¸€äº›çŸ©é˜µçš„åå­—å’Œç»“æ„ç›¸è¿‘å¯¼è‡´çš„ typoï¼Œä¸€å¼€å§‹å¹¶ä¸æ˜¯ bug-freeã€‚è¿˜å¥½åœ¨é˜…è¯» min-char-rnn çš„æ—¶å€™æ²¡æœ‰å·æ‡’å¿½ç•¥ gradient check(ing)ï¼Œå¤šäºå®ƒå‘Šè¯‰æˆ‘å®ç°æœ‰è¯¯ï¼Œé¿å…è®­ç»ƒå‡ºäººå·¥æ™ºéšœã€‚

æœ‰äº†ä¸Šé¢çš„åŸºç¡€ï¼Œä» 2 å±‚åˆ° N å±‚çš„è¿‡ç¨‹å°±æ¯”è¾ƒèƒ¸æœ‰æˆç«¹ã€‚N å±‚ çš„ vanilla RNN ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![The architecture of a n-layer RNN](./n-layer.png)

ç»“æ„ä¸Šï¼Œç¬¬äºŒå±‚ (h2) åˆ°ç¬¬ N å±‚ (hn) çš„ä¼ æ’­é€»è¾‘ä¸€æ ·ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ä¸€ä¸ªå¾ªç¯æ¥å®ç°ã€‚æœ‰äº†å‰é¢çš„ç»éªŒï¼Œè¿™æ¬¡å¾ˆå¿«å°±é€šè¿‡äº† gradient check(ing)ï¼Œæˆ‘çš„å®ç°å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_n_layers.py)æ‰¾åˆ°ï¼Œä»£ç ä¸­çš„å˜é‡å‘½åä¸å›¾ä¸­çš„æ ‡è¯†ä¿æŒä¸€è‡´ã€‚

## LSTM

ä» vanilla RNN åˆ° LSTM çš„è¿‡ç¨‹æ¯”æƒ³è±¡ä¸­æ›´è‰°éš¾ã€‚ç»è¿‡ç®€å•çš„å…³é”®è¯æ£€ç´¢ï¼Œä¸éš¾å‘ç° [nicodjimenez/lstm](https://github.com/nicodjimenez/lstm/) é¡¹ç›®ï¼Œç®€å•æ‰«ä¸€çœ¼å®ƒçš„ READMEï¼š

> A basic lstm network can be written from scratch in a few hundred lines of python, yet most of us have a hard time figuring out how lstm's actually work. The original Neural Computation paper is too technical for non experts. Most blogs online on the topic seem to be written by people who have never implemented lstm's for people who will not implement them either. Other blogs are written by experts ...

åŸºæœ¬å¯ä»¥æ–­å®šè¿™æ­£æ˜¯æˆ‘æ‰€éœ€ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œä½œè€…æåˆ°ä¸€ç¯‡ç»¼è¿°è®ºæ–‡ï¼š[A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/abs/1506.00019)ï¼Œç§°èµè¿™ç¯‡æ–‡ç« çš„åŒæ—¶ä¹Ÿæåˆ°è‡ªå·±çš„[å®ç°](https://github.com/nicodjimenez/lstm/blob/master/lstm.py)ä½¿ç”¨äº†æ–‡ç« ä¸­çš„æ•°å­¦ç¬¦å·ã€‚éƒ½å·²ç»çœ‹äº†ä¸€ç¯‡ matrix calculusï¼Œå†çœ‹ä¸€ç¯‡ç»¼è¿°æ–‡ç« é‚£åˆå¦‚ä½•ï¼Ÿäºæ˜¯åˆæ˜¯ä¸‰å››ä¸ªå°æ—¶è¿‡å»äº†...

è¯»æ¯•ï¼Œæˆ‘æœ€å¤§çš„æ„Ÿå—æ˜¯ï¼šæ¯”ã€Œç›´æ¥å­¦ä¹ å®ƒçš„ç»“æ„ï¼Œç„¶åå®ç°å®ƒã€æ›´é‡è¦çš„æ˜¯ç†è§£å®ƒçš„æå‡ºæ˜¯ä¸ºäº†è§£å†³ä»€ä¹ˆé—®é¢˜ã€‚å¦‚æœæ—¶é—´ä¸å¤Ÿï¼Œä¹Ÿå¯ä»¥é˜…è¯»è¿™ç¯‡åšå®¢ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)ã€‚

> ğŸ’¡åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œå°è¯•è‡ªå·±ç”¨ç¬”å’Œçº¸ç”»ä¸€äº› LSTM çš„ç»“æ„æœ‰åŠ©äºå¯¹ç½‘ç»œç»“æ„æ•´ä½“çš„ç†è§£

æœ‰äº†ä¸Šé¢çš„å‚¨å¤‡ï¼Œæ¥ä¸‹æ¥å°±å›åˆ° nicodjimenez/lstmï¼ŒæŠŠè„šæœ¬çœ‹æ‡‚å³å¯ã€‚äº‹ååˆ†æï¼Œä»æµ‹è¯•ç”¨ä¾‹å‡ºå‘æœ‰åŠ©äºæ›´å¿«åœ°ç†è§£ä»£ç ç»“æ„ã€‚ä½œè€…è‡ªå·±å†™äº†ä¸€ç¯‡åšå®¢ [Simple LSTM](https://nicodjimenez.github.io/2014/08/08/lstm.html) (å‘è¡¨æ—¶é—´æ—©äº Andrej çš„åšå®¢) ä»‹ç»ä»£ç ä¸­çš„åå‘ä¼ æ’­éƒ¨åˆ†ï¼Œä½†åªè¦å½»åº•ç†è§£äº† vanilla RNN çš„åå‘ä¼ æ’­åŸç†ï¼Œè¿™é‡Œçš„æ¨å¯¼åœ¨éš¾åº¦ä¸Šå¹¶æ²¡æœ‰æœ¬è´¨çš„æå‡ï¼Œåªæ˜¯ä¼ æ’­çš„æµå‘æ¯”åè€…å¤æ‚ä¸€äº›ã€‚

åœ¨èŠ±è´¹å°†è¿‘ä¸€ä¸ªå°æ—¶åï¼Œä¾è‘«èŠ¦ç”»ç“¢ï¼Œæˆ‘å†™å‡ºäº† [min-char-rnn çš„ lstm ç‰ˆæœ¬](https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_lstm.py)ã€‚åŒæ ·åœ°ï¼Œåˆ©ç”¨ gradient check(ing) ç¡®ä¿åå‘ä¼ æ’­éƒ¨åˆ†çš„é€»è¾‘å®ç°æ­£ç¡®ã€‚

## æ¢ç´¢ RNN çš„å¯èƒ½æ€§

TODO

## å‚è€ƒèµ„æ–™

* [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Github: karpathy](https://github.com/karpathy), [char-rnn](https://github.com/karpathy/char-rnn), [min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086)
* [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/index.html)
* [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)
* [UFLDL Tutorial](http://ufldl.stanford.edu/tutorial/), [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
* [eliben/deep-learning-samples/min-char-rnn](https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py)
*  [nicodjimenez/lstm](https://github.com/nicodjimenez/lstm/)
* [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [ZhengHe-MD/replay-nn-tutorials/min-char-rnn](https://github.com/ZhengHe-MD/replay-nn-tutorials/tree/main/min-char-rnn)
