<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhenghe-md.github.io","root":"/blog/","images":"/blog/images","scheme":"Mist","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"remove","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/blog/js/config.js"></script>
<meta name="description" content="What I cannot create, I do not understand. -- Richard Feynman  Andrej Karpathy 在 2015 年发表了题为 The Unreasonable Effectiveness of Recurrent Neural Networks 的博客，并配套开源了其中实验所用的 char-rnn 代码仓库，以及用 numpy 手写的">
<meta property="og:type" content="article">
<meta property="og:title" content="从头开始实现 RNN">
<meta property="og:url" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/index.html">
<meta property="og:site_name" content="ZhengHe">
<meta property="og:description" content="What I cannot create, I do not understand. -- Richard Feynman  Andrej Karpathy 在 2015 年发表了题为 The Unreasonable Effectiveness of Recurrent Neural Networks 的博客，并配套开源了其中实验所用的 char-rnn 代码仓库，以及用 numpy 手写的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://imgs.xkcd.com/comics/standards.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/one-layer.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/gradient-check.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/two-layer.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/n-layer.png">
<meta property="article:published_time" content="2022-02-20T15:03:20.000Z">
<meta property="article:modified_time" content="2022-07-05T11:38:56.860Z">
<meta property="article:author" content="ZhengHe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://imgs.xkcd.com/comics/standards.png">


<link rel="canonical" href="https://zhenghe-md.github.io/blog/2022/02/20/rnn/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zhenghe-md.github.io/blog/2022/02/20/rnn/","path":"2022/02/20/rnn/","title":"从头开始实现 RNN"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>从头开始实现 RNN | ZhengHe</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172943223-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-172943223-1","only_pageview":false}</script>
  <script src="/blog/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container {
  overflow: auto hidden;
}

mjx-container + br {
  display: none;
}
</style><link rel="alternate" href="/blog/atom.xml" title="ZhengHe" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ZhengHe</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ZhengHe-MD" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhenghe-md.github.io/blog/2022/02/20/rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="ZhengHe">
      <meta itemprop="description" content="郑鹤的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZhengHe">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从头开始实现 RNN
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-20 15:03:20" itemprop="dateCreated datePublished" datetime="2022-02-20T15:03:20+00:00">2022-02-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-05 11:38:56" itemprop="dateModified" datetime="2022-07-05T11:38:56+00:00">2022-07-05</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/blog/2022/02/20/rnn/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/02/20/rnn/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>What I cannot create, I do not understand. -- Richard Feynman</p>
</blockquote>
<p>Andrej Karpathy 在 2015 年发表了题为 <a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 的博客，并配套开源了其中实验所用的 <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">char-rnn 代码仓库</a>，以及用 numpy 手写的 <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/d4dee566867f8291f086">gist: min-char-rnn</a>，阅读过后受益良多。于是最近花了一些时间做了下面这些事情：</p>
<ol type="1">
<li><p>逐行理解 min-char-rnn，即 vanilla RNN</p></li>
<li><p>实现 N 层 vanilla RNN</p></li>
<li><p>实现 LSTM (Long Short-Term Memory) RNN</p></li>
<li><p>探索 RNN 的可能性</p>
<ul>
<li>Paul Graham generator</li>
<li>三国演义</li>
<li>老友记</li>
<li>Kubernetes 源码</li>
<li>超级丹的技战术</li>
</ul></li>
</ol>
<p>整个探索过程对我而言充满了趣味和挑战，并且在实践中令人激动地首次使用微积分的知识。尽管这只是深度学习的冰山一角，但足以让一名主营业务为服务端开发的软件工程师感到激动不已，于是便有了这篇博客，将这个过程记录下来。</p>
<span id="more"></span>
<p>在构思这篇博客前，我曾经想过写一个完整的、细致入微的、保姆式的从 0 - 1 实现 RNN 的教程。后来发现，在我自己阅读、理解和实现的过程中，至少投入数十个小时阅读大量资料，期间并没有发现任何一篇文章能做到这点，那么我又如何能期望写一篇博客做到这点？</p>
<figure>
<img src="https://imgs.xkcd.com/comics/standards.png" alt="" /><figcaption>xkcd-standards</figcaption>
</figure>
<p>那什么东西是值得分享的呢？我思考的结果是「学习路径」。在遇到理解障碍时，我尝试寻找并阅读了哪些资料，花费了多长时间，有什么心得体会，这些也许能够给到读者灵感。</p>
<blockquote>
<p>⚠️ 本文假设读者有一定的深度学习理论基础和实践经验，同时阅读过上述的博客和脚本。但如果不介意可能出现的理解障碍，也欢迎继续阅读本文。</p>
<p>💡 一位热心网友就着 min-char-rnn 给出了一个带注释的<a target="_blank" rel="noopener" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py">版本</a>，阅读后者的难度会更小一些</p>
</blockquote>
<h2 id="理解-min-char-rnn">理解 min-char-rnn</h2>
<p>min-char-rnn 实现的是单层的 vanilla RNN，其整体结构如下图所示：</p>
<figure>
<img src="./one-layer.png" alt="" /><figcaption>The architecture of min-char-rnn</figcaption>
</figure>
<h3 id="前向传播">前向传播</h3>
<p>以下是源码中的前向传播部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(<span class="built_in">len</span>(inputs)):</span><br><span class="line">    xs[t] = np.zeros((vocab_size,<span class="number">1</span>)) <span class="comment"># encode in 1-of-k representation</span></span><br><span class="line">    xs[t][inputs[t]] = <span class="number">1</span></span><br><span class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-<span class="number">1</span>]) + bh) <span class="comment"># hidden state</span></span><br><span class="line">    ys[t] = np.dot(Why, hs[t]) + by <span class="comment"># unnormalized log probabilities for next chars</span></span><br><span class="line">    ps[t] = np.exp(ys[t]) / np.<span class="built_in">sum</span>(np.exp(ys[t])) <span class="comment"># probabilities for next chars</span></span><br><span class="line">    loss += -np.log(ps[t][targets[t],<span class="number">0</span>]) <span class="comment"># softmax (cross-entropy loss)</span></span><br></pre></td></tr></table></figure>
<p>通常比较精巧的地方就是令人费解的地方，这里比较精巧的自然是 CrossEntropy 的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss += -np.log(ps[t][targets[t],<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>CrossEntropy 计算的是两个概率分布的差异程度，差异越大，熵值越大。这里的两个概率分布分别是「RNN 预测的下一个字符的概率分布」和「实际下一个字符的概率分布」，在监督学习过程中，后者是一件已经确定的事情，所以它的概率分布是 [0, 0, ..., 1, 0, 0]，即除一个确定字符的出现概率为 1 外，剩余字符的出现概率皆为 0。于是最终对损失函数产生贡献的只有某一个字符出现的概率，即这里的 <code>ps[t][targets[t], 0]</code>。</p>
<h3 id="反向传播">反向传播</h3>
<p>以下是源码中的反向传播部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(xrange(<span class="built_in">len</span>(inputs))):</span><br><span class="line">    dy = np.copy(ps[t])</span><br><span class="line">    dy[targets[t]] -= <span class="number">1</span> <span class="comment"># backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span></span><br><span class="line">    dWhy += np.dot(dy, hs[t].T)</span><br><span class="line">    dby += dy</span><br><span class="line">    dh = np.dot(Why.T, dy) + dhnext <span class="comment"># backprop into h</span></span><br><span class="line">    dhraw = (<span class="number">1</span> - hs[t] * hs[t]) * dh <span class="comment"># backprop through tanh nonlinearity</span></span><br><span class="line">    dbh += dhraw</span><br><span class="line">    dWxh += np.dot(dhraw, xs[t].T)</span><br><span class="line">    dWhh += np.dot(dhraw, hs[t-<span class="number">1</span>].T)</span><br><span class="line">    dhnext = np.dot(Whh.T, dhraw)</span><br></pre></td></tr></table></figure>
<p>如果你像我一样只对数学分析中最基本的「单变量求导」还留存有一定的记忆，那看这段代码会有种「表面上好像能看懂，仔细思考又觉得哪里对不上」的感觉。具体地说，我们很容易看出来这些代码是在将损失函数的变化按相反的方向一步一步地倒推回去，再加上脑海中尚存的单变量「链式求导」规则，似乎一切顺理成章。但「细思极恐」，这里面有标量、有向量，还有矩阵，什么是「标量对向量求导」？什么是「标量对矩阵求导」？什么是「向量对向量求导」？什么是「矩阵对向量求导」？什么是「矩阵对矩阵求导」？我们学过「向量求导」或者「矩阵求导」吗？</p>
<p>为了消解脑海中尚不清晰的地方，我通过搜索引擎，以类似「matrix calculus for engineer」的关键词，找到了一篇长文：<a target="_blank" rel="noopener" href="https://explained.ai/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning</a>，它正好是为像我这样<strong>只记得「单变量求导」的人</strong>量身定做的资料。于是我集中精力花了 3-4 个小时把全文通读，随后在笔记本上将代码中的几个标量 (损失函数)、向量、矩阵间的求导都推了一遍，终于云开雾散，事实证明这样的付出十分值得。</p>
<p>Andrej 在反向传播 Softmax + CrossEntropy 时写了个备注：</p>
<blockquote>
<p>backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</p>
</blockquote>
<p>令人沮丧的是，当你仔细阅读这部分内容，会发现这样一句话：</p>
<blockquote>
<p>It’s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out...</p>
</blockquote>
<p>真正看懂它，是我在阅读完上面那篇关于 Matrix Calculus 的扫盲文章，紧接着继续阅读这篇博客 <a target="_blank" rel="noopener" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">The Softmax function and its derivative</a> 之后。<strong>最终的答案很具有美感，但推导的过程很需要耐心</strong>。</p>
<p>值得一提的是，这里在训练的时候还使用了「teacher forcing」的技巧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dy = np.copy(ps[t])</span><br><span class="line">dy[targets[t]] -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>即不管 RNN 在每次 unroll 都使用标准答案，而不是自身学到的结果 <code>np.argmax(ps[t])</code>。在其它教程中，我也看到过人们会给「teacher forcing」施加一个概率，就像 <code>dropout_p</code>，因为在推理过程中不会有任何字符的参考结果。据说通过降低这个概率可以一定程度上避免「学生在实际解题时过分依赖老师」。</p>
<h3 id="gradient-check">Gradient check</h3>
<p>在 min-char-rnn 脚本下的第一个评论，就是 Andrej 自己的：</p>
<figure>
<img src="./gradient-check.png" alt="" /><figcaption>gradient check</figcaption>
</figure>
<p>刚看到这段代码，我完全不知道 gradient check 是在做什么，自然也无法理解这段代码的含义。不过在搜索引擎的帮助下，我还是找到了斯坦福大学开放的教程 <a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/">Unsupervised Feature Learning and Deep Learning</a>，其中一节正是介绍 <a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/">Debugging: Gradient Checking</a>。所谓 gradient check(ing) 其实就是对比「通过反向传播计算出来的梯度」与「利用导数定义近似计算得到的梯度」，来判断自己的反向传播阶段代码是否写对了，是工程师构建神经网络时常用的一种简单有效的 debug 方法。由于每次训练都可能非常耗时，而且一些问题即便存在，表面上也可能看着一切正常。在构建完神经网络后，开始训练之前，执行 gradient check(ing) 很有必要。</p>
<p>在实现 2 层、 n 层 RNN 的过程中，我就利用 gradient check(ing) 成功发现了代码中的若干逻辑问题。这些问题不会导致程序崩溃，通过传统的「眼神调试」、「print 调试」、「单点调试」都极难发现。</p>
<h2 id="n-层-vanilla-rnn">N 层 vanilla RNN</h2>
<p>在阅读 min-char-rnn 时，我在心里就萌生了一个想法：能否依葫芦画瓢直接用 numpy 实现一个 N 层 vanilla RNN？但步子太大容易扯着蛋，先挑战一个 2 层的 vanilla RNN：</p>
<figure>
<img src="./two-layer.png" alt="" /><figcaption>The architecture of a 2-layer RNN</figcaption>
</figure>
<p>在结构上，2 层的 vanilla RNN 多了一个 hidden layer，第二层 (h2) 的输入就是第一层 (h1) 的输出，主要区别在于两层的输入维度不同，因为这点不同，在代码中需要分开实现它们的传播逻辑。我的实现可以在<a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_two_layers.py">这里</a>找到，代码中的变量命名与图中的标识保持一致。实现基本逻辑大约花费了半个小时时间，但由于对 numpy 的不熟悉以及一些矩阵的名字和结构相近导致的 typo，一开始并不是 bug-free。还好在阅读 min-char-rnn 的时候没有偷懒忽略 gradient check(ing)，多亏它告诉我实现有误，避免训练出人工智障。</p>
<p>有了上面的基础，从 2 层到 N 层的过程就比较胸有成竹。N 层 的 vanilla RNN 结构如下图所示：</p>
<figure>
<img src="./n-layer.png" alt="" /><figcaption>The architecture of a n-layer RNN</figcaption>
</figure>
<p>结构上，第二层 (h2) 到第 N 层 (hn) 的传播逻辑一样，因此可以通过一个循环来实现。有了前面的经验，这次很快就通过了 gradient check(ing)，我的实现可以在<a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_n_layers.py">这里</a>找到，代码中的变量命名与图中的标识保持一致。</p>
<h2 id="lstm">LSTM</h2>
<p>从 vanilla RNN 到 LSTM 的过程比想象中更艰难。经过简单的关键词检索，不难发现 <a target="_blank" rel="noopener" href="https://github.com/nicodjimenez/lstm/">nicodjimenez/lstm</a> 项目，简单扫一眼它的 README：</p>
<blockquote>
<p>A basic lstm network can be written from scratch in a few hundred lines of python, yet most of us have a hard time figuring out how lstm's actually work. The original Neural Computation paper is too technical for non experts. Most blogs online on the topic seem to be written by people who have never implemented lstm's for people who will not implement them either. Other blogs are written by experts ...</p>
</blockquote>
<p>基本可以断定这正是我所需。在这个项目中，作者提到一篇综述论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.00019">A Critical Review of Recurrent Neural Networks for Sequence Learning</a>，称赞这篇文章的同时也提到自己的<a target="_blank" rel="noopener" href="https://github.com/nicodjimenez/lstm/blob/master/lstm.py">实现</a>使用了文章中的数学符号。都已经看了一篇 matrix calculus，再看一篇综述文章那又如何？于是又是三四个小时过去了...</p>
<p>读毕，我最大的感受是：比「直接学习它的结构，然后实现它」更重要的是理解它的提出是为了解决什么问题。如果时间不够，也可以阅读这篇博客 <a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>。</p>
<blockquote>
<p>💡在学习的过程中，尝试自己用笔和纸画一些 LSTM 的结构有助于对网络结构整体的理解</p>
</blockquote>
<p>有了上面的储备，接下来就回到 nicodjimenez/lstm，把脚本看懂即可。事后分析，从测试用例出发有助于更快地理解代码结构。作者自己写了一篇博客 <a target="_blank" rel="noopener" href="https://nicodjimenez.github.io/2014/08/08/lstm.html">Simple LSTM</a> (发表时间早于 Andrej 的博客) 介绍代码中的反向传播部分，但只要彻底理解了 vanilla RNN 的反向传播原理，这里的推导在难度上并没有本质的提升，只是传播的流向比后者复杂一些。</p>
<p>在花费将近一个小时后，依葫芦画瓢，我写出了 <a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_lstm.py">min-char-rnn 的 lstm 版本</a>。同样地，利用 gradient check(ing) 确保反向传播部分的逻辑实现正确。</p>
<h2 id="探索-rnn-的可能性">探索 RNN 的可能性</h2>
<p>实现完玩具版 RNN，理解已经足够到位，可以开始做一些有意思的事情了。本文开篇提到过，Andrej 开放了他做实验所使用的 <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">char-rnn</a> 代码仓库，提供了 RNN、LSTM 和 GRU (另一种 RNN) 的实现以及一整套训练、推理的流程。在项目 <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn/blob/master/Readme.md">README.md</a> 中，Andrej 又推荐了来自 Justin Johnson 优化性能后的版本：<a target="_blank" rel="noopener" href="https://github.com/jcjohnson/torch-rnn">torch-rnn</a>，后者甚至提供了<a target="_blank" rel="noopener" href="https://github.com/crisbal/docker-torch-rnn">容器化支持</a>。在花费了若干小时 (尴尬的是，大部分时间花在了等待 <code>docker pull</code> 上) 搭建好 GPU 环境后，终于可以开始正式探索。</p>
<blockquote>
<p>💡如果你手边有 GPU，搭建环境时可能还需要这些资料：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/">cuda-installation-guide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker2</a></li>
</ul>
<p>最后用 nvidia-docker 启动时可以把 torch-rnn 项目的本地路径用 Docker 的 volume 参数与容器内部的 <code>~/torch-rnn</code> 路径建立双向绑定，这样就可以在容器里自由地训练模型。</p>
</blockquote>
<h3 id="paul-graham-generator">Paul Graham generator</h3>
<p>先复现一下 Andrej 博客中的第一个实验，将 Paul Graham 的 essays 抓取到本地 (<a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/paulgraham/dump_paulgraham_essays.py">我的脚本</a>)，然后将其合并成一个文本文件，送入模型。实验参数与 Andrej 的保持一致，训练 50 个 epochs，下面是用模型采样出的段落示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">So there even a list of few things you&#x27;re supposed to concipe of </span><br><span class="line">different startups—you need to convince customers that new gets </span><br><span class="line">painting is one of them at startups without taking that. They&#x27;re a </span><br><span class="line">rare concopporation, that&#x27;s within college. To a real way to addict </span><br><span class="line">you, but what you&#x27;re expected.</span><br><span class="line">If you&#x27;re capped in several programmers, what proves them-defend </span><br><span class="line">what you&#x27;re working about. But you don&#x27;t notice based on your generation: </span><br><span class="line">since he doesn&#x27;t realize it: all much shouldn&#x27;t be just that that, </span><br><span class="line">that is short of this actual audience. Open study new people </span><br><span class="line">they have shifting up a city. [ 7 ] This surprises still say here told </span><br><span class="line">it&#x27;s a platazine. There seem to degree why formidable things we tend to </span><br><span class="line">volunteregal startups elity is in Grisk Fortle. Sometomes The </span><br><span class="line">other generations were proportionate at real implications to cogain.</span><br><span class="line">If there are working off you&#x27;re working on simply to like that sharp </span><br><span class="line">it horter than you. How so hote-wealth to treates back of friends </span><br><span class="line">they to be doing. Most nived in a number is to concied our fioks companies.</span><br></pre></td></tr></table></figure>
<p>中间的这句话让我很是喜欢：</p>
<blockquote>
<p>To a real way to addict you, but what you're expected.</p>
</blockquote>
<p>可以翻译成「让你上瘾，甚至用你期望的方式」吗？</p>
<h3 id="三国演义">三国演义</h3>
<p>罗贯中的版权现在已经不受法律保护，于是我轻而易举地在百度搜到了三国演义的全文 txt。让我们一起看看 RNN 的文言文作品：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">太史云长闻之，即遣人马往全徐聚及关缚。斗时一将军马，欲取平郡；</span><br><span class="line">操轻勒弃衣弓甲，来夜身中。原马玄德回寨新中，勒法东飞未还，因虽带明一林而到。</span><br><span class="line">韦众将令军接后，折声桥剑于穿着。备入城十里，赵云拍马皆进，被给来迎之，</span><br><span class="line">布会绳帐落下命，引一马军无弩而走。百凉五路，杀之处，走由精营马也。</span><br><span class="line">次日，如柴把拥山白下阵。待我三月，手力顺从，直至白配。两十白得小枪粮住乱，</span><br><span class="line">砍拜军士相招烈，兴阵后料，高边卧打宝后一齐出，右图山草，山行至溪。</span><br><span class="line">又所通人打身平阵，兼见剿伙。马超在前扎下兵，一面有军卧洛百人，只被白首视之。</span><br><span class="line">操为归锋，丕败宣桥进走曰：“宋大浩于来水，未如能敢施；无歹染之兵，</span><br><span class="line">彼得生动谗以理辞己下大机也！”表云：“不可此常之，今与何意！”</span><br><span class="line">随从吕旷令西凉姜布有荆州，箭政跪身，军马挺南。曹操大不告离，何守而降。先主非士，又命关看。</span><br><span class="line"></span><br><span class="line">    甫犹报关曹军师武俱山锋，休见玄德。坚功意久，立了刘备，嘉乃置臣谋长耳。</span><br><span class="line">荀瑁曰：““吴悌实雄违，休得逆我，难来用将。昨恐萧通为运。吾不以向计也，各有不可，名之。”</span><br><span class="line">瑜惊曰：“孤奉某驱反星不和！”丕大惊，乃下一面中调赖居，哨待曹公老父，乃诸县呈“</span><br><span class="line">热荒子因外谭自，引军人骑法。国秋引兵至涪坏寨，叫肃又退，威不想送芝如决，</span><br><span class="line">可使将过了玄德。军士义车绑交郡看。当日山戈从石汉中去了，言玄德已得家草，</span><br><span class="line">孙瓒便孙乾守而征。忽然一帜四山诸万将倾否：“两谋人皆裨宝饿河，四海有过所力；</span><br><span class="line">左看在金中，掣徐州皆上破营。祖首朝廷不受；势四股盟，并设猖峡：</span><br><span class="line"></span><br><span class="line">    却说植驱来，饱接也见，道感温校。只闻吾激透言城不然。</span><br><span class="line"></span><br><span class="line">    且军马报徐州渡寨，再收将催投“将分布、乔掌、韩仲。各、张仪刀手于军马。</span><br><span class="line">少败进，操问庞德曰：“汝平有真妙矣！”阿视之，拜便商议。瓒部商情：左右看鲁为太，</span><br><span class="line">计偿衡挟，提乏畏中。表料第氏受之情，未胜者报孔明，用然天下。</span><br><span class="line">孔明无在车仗至条厅亡，隐教转到周上。后人有诗赞一徒方诺，毗进入府基。</span><br><span class="line">先说孔明曰：“杀帝刘业，与卿六锋也！”孔明曰：“汝来说丞相夫战，亦何鼓降？”二人便同先阵器。</span><br></pre></td></tr></table></figure>
<p>你是否也像我一样认认真真地读了一遍？虽然不知道它在写什么，但 RNN 对文言文的掌握我还是自愧不如。</p>
<h3 id="老友记">老友记</h3>
<p>一位网友在 Github 上维护了老友记全 10 季的剧本：<a target="_blank" rel="noopener" href="https://github.com/fangj/friends">fangj/friends</a>，他应该不会想到有一天这个仓库会变成 AI 的「饲料」？写了一个脚本简单将 html 处理成了普通文本 (去掉标签，unescape 一些特殊字符)，送进模型训练。下面是得到的剧本节选：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[Scene: Chandler and Joey&#x27;s, Phoebe is answering from some gateen, the same assarent duck is</span><br><span class="line">smile.]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Ross: Seriously, I can’t…I play larny?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">All: Oh, guys. This, this is where it’s such some Agenton on the missay I could take</span><br><span class="line">the bean in your side to me.</span><br><span class="line"></span><br><span class="line">Phoebe: What? What do you think he’s gonna see you?</span><br><span class="line"></span><br><span class="line">Rachel: Really?! The only bay idea, but that’s okay! You gotta get rome,</span><br><span class="line">tell me!</span><br><span class="line"></span><br><span class="line">Rachel: Wow! What are you laughing after anything women?</span><br><span class="line"></span><br><span class="line">Ross: Monnica told me about! This is beautiful that turts familiar. So I get a</span><br><span class="line">button, fine. What are you doing?</span><br><span class="line"></span><br><span class="line">Ross: Da me. (Pretends in door) I decided to cold use that.</span><br><span class="line"></span><br><span class="line">Rachel: Whoa, whoa whoa, day! What a duving dollars!!!!</span><br></pre></td></tr></table></figure>
<p>可以看出，RNN 掌握了剧本的结构：场景、人物、对话、动作。虽然对话看起来有些空洞，但令人惊讶的是它学会了 Rachel 的语气：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Rachel: Whoa, whoa whoa, day! What a duving dollars!!!!</span><br></pre></td></tr></table></figure>
<p>从「 <code>a</code> 和 <code>dollars</code> 同时出现」可以了解到它的语法还未学到位，不过写成这样还不值得我们鼓励一下？</p>
<h3 id="kubernetes">Kubernetes</h3>
<p><a target="_blank" rel="noopener" href="https://copilot.github.com/">Codepilot</a> 和 <a target="_blank" rel="noopener" href="https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode">AlphaCode</a> 已经向工程师们证明了 AI 写代码的能力，我也想看看 3 层 LSTM 能做到什么样的程度。于是我 clone 了 Kubernetes 仓库，将所有 Golang 源码聚合 (cat) 成一个文本 (你猜猜 Kubernetes 的 codebase 所占的空间有多大？评论区告诉你)，送进 torch-rnn，训练两个小时，采样出了下面这段代码：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reporter kubeconfig bootstrap implementation back of ne non-best or proto</span></span><br><span class="line"><span class="comment">// callered present to a new iterator that are build to</span></span><br><span class="line"><span class="comment">// seriily region.</span></span><br><span class="line"><span class="keyword">type</span> Instances <span class="keyword">struct</span> &#123;</span><br><span class="line">	Interface framework.ClientVolumeInterface</span><br><span class="line">	wait MustInfoDefaultCreatePrivilegedValue</span><br><span class="line">	HostDNS    <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">	<span class="comment">// Needed inshelt is data from bucket for checks, if the double with</span></span><br><span class="line">	<span class="comment">// this address group and needs used to less 4anology and https://github.com/policy/existing-storage/using/recevable-src image/shtge efility and intendaroping</span></span><br><span class="line">	<span class="comment">// store</span></span><br><span class="line">	decisionKey *<span class="keyword">string</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// Prefer operations that successfully at I registry. See methods</span></span><br><span class="line">	<span class="comment">// instance (documentations) the console set4 are used.</span></span><br><span class="line">	Codes <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewPathParameter address taints of Server automatically.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pl *NormalGanglIngress)</span> <span class="title">GetAll</span><span class="params">(data <span class="keyword">interface</span>&#123;&#125;, contains *stderr.IsState)</span> <span class="params">(<span class="keyword">bool</span>, error)</span></span> &#123;</span><br><span class="line">	key := &amp;zs.FreeCheck&#123;&#125;</span><br><span class="line">	_, err := d.info.CommandStatus(ctx)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	error := field.ErrorTypes()</span><br><span class="line">	<span class="keyword">return</span> withRegisterBackoff(indexing)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// JSONNM currently directory was returned in it deleting the recorder and certificates.</span></span><br><span class="line"><span class="keyword">type</span> Attacher <span class="keyword">struct</span> &#123;</span><br><span class="line">	obj <span class="keyword">uint32</span></span><br><span class="line">	cidr []*Decls.ParseMessage</span><br><span class="line">	sgip          io.SharedInformer</span><br><span class="line">	Snapshot       *Token</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *symconfig)</span> <span class="title">DescribeRestore</span><span class="params">(ctx context.Context, m file.Info)</span> <span class="params">(<span class="keyword">bool</span>, error)</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从最后一个函数 <code>DescribeRestore</code> 可以看到，我们的模型还没发现返回参数的数量需要与函数定义保持一致。但至少这些代码被 Golang 的语法高亮插件认可了！甚至它还学会了写注释。同样有意思的是，我在平时工作中写代码时，常常需要依赖 <code>gofmt</code> 或 <code>goimports</code> 这样的工具自动格式化代码，比如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Attacher <span class="keyword">struct</span> &#123;</span><br><span class="line">	obj <span class="keyword">uint32</span></span><br><span class="line">	cidr []*Decls.ParseMessage</span><br><span class="line">	sgip          io.SharedInformer</span><br><span class="line">	Snapshot       *Token</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过格式化后，会变成：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Attacher <span class="keyword">struct</span> &#123;</span><br><span class="line">	obj      <span class="keyword">uint32</span></span><br><span class="line">	cidr     []*Decls.ParseMessage</span><br><span class="line">	sgip     io.SharedInformer</span><br><span class="line">	Snapshot *Token</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的代码来看，我们的 RNN 似乎也需要这样的工具。</p>
<h3 id="超级丹的技战术">超级丹的技战术</h3>
<p>这是一个我尚未有精力去完成的实验，因为它的前期数据处理工作量太大，在这里我仅介绍一下想法：</p>
<p>喜欢看/打羽毛球的朋友都知道，球员在场上步伐 (并步、跑步、交叉步等)、击球方式 (高、吊、杀、搓、放等)、球的落点 (前、中、后、左、中、右 9 个点) 的一系列选择，构成球员的技战术打法。这些选择来自于球员对场上局势的判断 (之前的来回、对方球员的选择以及双方的身心状态)。</p>
<p>如果我们能将这些信息编码成离散的状态，就能将羽毛球比赛抽象成时序数据。要学就要从最好的学，所以我的想法是：获取林丹巅峰时期的所有比赛视频，记录下他和所有对手在每个回合做出的球路选择，然后送进 RNN。是不是有助于新的运动员快速掌握「超级丹」的球路？希望我有一天能有闲暇时间来完成这项工作。如果看到这篇博客的你是国家羽毛球训练中心的工作人员，欢迎联系我， just kidding : )。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy">Github: karpathy</a>, <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">char-rnn</a>, <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/d4dee566867f8291f086">min-char-rnn</a></li>
<li><a target="_blank" rel="noopener" href="https://explained.ai/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">The Softmax function and its derivative</a></li>
<li><a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/">UFLDL Tutorial</a>, <a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/">Debugging: Gradient Checking</a></li>
<li><a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py">eliben/deep-learning-samples/min-char-rnn</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/nicodjimenez/lstm/">nicodjimenez/lstm</a></li>
<li><a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/tree/main/min-char-rnn">ZhengHe-MD/replay-nn-tutorials/min-char-rnn</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/blog/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>
  </div>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blog/2022/02/07/quick-translate/" rel="prev" title="翻译能有多快？在 Mac 上实现一键翻译">
                  <i class="fa fa-chevron-left"></i> 翻译能有多快？在 Mac 上实现一键翻译
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/blog/2022/02/26/compound-interest-in-life/" rel="next" title="复利的隐喻">
                  复利的隐喻 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZhengHe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/blog/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"zhenghe-hexo-blog","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/blog/js/third-party/comments/disqus.js"></script>

</body>
</html>
