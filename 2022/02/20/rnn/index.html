<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zhenghe-md.github.io","root":"/blog/","images":"/blog/images","scheme":"Mist","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"remove","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/blog/js/config.js"></script>
<meta name="description" content="What I cannot create, I do not understand. -- Richard Feynman  Andrej Karpathy 在 2015 年发表了题为 The Unreasonable Effectiveness of Recurrent Neural Networks 的博客，并配套开源其中实验所用的char-rnn 代码仓库，以及用 numpy 手写的 gi">
<meta property="og:type" content="article">
<meta property="og:title" content="从头开始实现 RNN">
<meta property="og:url" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/index.html">
<meta property="og:site_name" content="ZhengHe">
<meta property="og:description" content="What I cannot create, I do not understand. -- Richard Feynman  Andrej Karpathy 在 2015 年发表了题为 The Unreasonable Effectiveness of Recurrent Neural Networks 的博客，并配套开源其中实验所用的char-rnn 代码仓库，以及用 numpy 手写的 gi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/one-layer.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/gradient-check.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/two-layer.png">
<meta property="og:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/n-layer.png">
<meta property="article:published_time" content="2022-02-20T15:03:20.000Z">
<meta property="article:modified_time" content="2022-02-20T12:41:30.765Z">
<meta property="article:author" content="ZhengHe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhenghe-md.github.io/blog/2022/02/20/rnn/one-layer.png">


<link rel="canonical" href="https://zhenghe-md.github.io/blog/2022/02/20/rnn/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://zhenghe-md.github.io/blog/2022/02/20/rnn/","path":"2022/02/20/rnn/","title":"从头开始实现 RNN"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>从头开始实现 RNN | ZhengHe</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172943223-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-172943223-1","only_pageview":false}</script>
  <script src="/blog/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container {
  overflow: auto hidden;
}

mjx-container + br {
  display: none;
}
</style><link rel="alternate" href="/blog/atom.xml" title="ZhengHe" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ZhengHe</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ZhengHe-MD" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhenghe-md.github.io/blog/2022/02/20/rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="ZhengHe">
      <meta itemprop="description" content="郑鹤的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZhengHe">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从头开始实现 RNN
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-02-20 15:03:20 / 修改时间：12:41:30" itemprop="dateCreated datePublished" datetime="2022-02-20T15:03:20+00:00">2022-02-20</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/blog/2022/02/20/rnn/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/02/20/rnn/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>What I cannot create, I do not understand. -- Richard Feynman</p>
</blockquote>
<p>Andrej Karpathy 在 2015 年发表了题为 <a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 的博客，并配套开源其中实验所用的<a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">char-rnn 代码仓库</a>，以及用 numpy 手写的 <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/d4dee566867f8291f086">gist: min-char-rnn</a>，阅读过后受益良多。于是这两天花了些时间：</p>
<ol type="1">
<li>逐行理解 min-char-rnn</li>
<li>基于理解和原脚本实现了 2-layer 和 n-layer 的 RNN</li>
</ol>
<p>整个探索过程充满了趣味和挑战，尤其是对于一位主营服务端开发的工程师，因此特意将这个过程记录下来。</p>
<span id="more"></span>
<blockquote>
<p>⚠️ 本文假设读者有一定的深度学习理论基础和实践经验，同时阅读过上述的博客和脚本。但如果不介意可能出现的理解障碍，也欢迎继续阅读本文。</p>
</blockquote>
<h2 id="理解-min-char-rnn">理解 min-char-rnn</h2>
<p>min-char-rnn 实现的是单层的 Vanilla RNN，其整体结构如下图所示：</p>
<figure>
<img src="./one-layer.png" alt="" /><figcaption>The architecture of min-char-rnn</figcaption>
</figure>
<blockquote>
<p>💡 一位热心网友就着 min-char-rnn 给出了一个带注释的<a target="_blank" rel="noopener" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py">版本</a>，阅读后者的难度会更小一些</p>
</blockquote>
<h3 id="前向传播">前向传播</h3>
<p>以下是源码中的前向传播部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(<span class="built_in">len</span>(inputs)):</span><br><span class="line">    xs[t] = np.zeros((vocab_size,<span class="number">1</span>)) <span class="comment"># encode in 1-of-k representation</span></span><br><span class="line">    xs[t][inputs[t]] = <span class="number">1</span></span><br><span class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-<span class="number">1</span>]) + bh) <span class="comment"># hidden state</span></span><br><span class="line">    ys[t] = np.dot(Why, hs[t]) + by <span class="comment"># unnormalized log probabilities for next chars</span></span><br><span class="line">    ps[t] = np.exp(ys[t]) / np.<span class="built_in">sum</span>(np.exp(ys[t])) <span class="comment"># probabilities for next chars</span></span><br><span class="line">    loss += -np.log(ps[t][targets[t],<span class="number">0</span>]) <span class="comment"># softmax (cross-entropy loss)</span></span><br></pre></td></tr></table></figure>
<p>通常比较巧妙的地方就是令人费解的地方，这里比较巧妙的自然是 CrossEntropy 的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss += -np.log(ps[t][targets[t],<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>CrossEntropy 计算的是两个概率分布的差异程度，差异越大，熵值越大。这里的两个概率分布分别是「RNN 预测的下一个字符的概率分布」和「实际下一个字符的概率分布」，在监督学习过程中，后者是一件已经确定的事情，所以它的概率分布是 [0, 0, ..., 1, 0, 0]，即除一个确定字符的出现概率为 1 外，剩余字符的出现概率皆为 0。于是最终对损失函数产生贡献的只有一个字符出现的概率，即这里的 <code>ps[t][targets[t], 0]</code>。</p>
<h3 id="反向传播">反向传播</h3>
<p>以下是源码中的反向传播部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(xrange(<span class="built_in">len</span>(inputs))):</span><br><span class="line">    dy = np.copy(ps[t])</span><br><span class="line">    dy[targets[t]] -= <span class="number">1</span> <span class="comment"># backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span></span><br><span class="line">    dWhy += np.dot(dy, hs[t].T)</span><br><span class="line">    dby += dy</span><br><span class="line">    dh = np.dot(Why.T, dy) + dhnext <span class="comment"># backprop into h</span></span><br><span class="line">    dhraw = (<span class="number">1</span> - hs[t] * hs[t]) * dh <span class="comment"># backprop through tanh nonlinearity</span></span><br><span class="line">    dbh += dhraw</span><br><span class="line">    dWxh += np.dot(dhraw, xs[t].T)</span><br><span class="line">    dWhh += np.dot(dhraw, hs[t-<span class="number">1</span>].T)</span><br><span class="line">    dhnext = np.dot(Whh.T, dhraw)</span><br></pre></td></tr></table></figure>
<p>如果你像我一样只对数学分析中最基本的「单变量求导」还留存有一定的记忆，那看这段代码会有种「表面上好像能看懂，仔细思考又觉得哪里对不上」的感觉。具体地说，我们很容易看出来这些代码是在将损失函数的变化按相反的方向一步一步地倒推回去，再加上脑海中尚存的单变量「链式求导」规则，似乎一切顺理成章。但细思极「恐」，这里面有标量、有向量，还有矩阵，什么是「标量对矩阵求导」？什么是「向量对向量求导」？什么是「矩阵对向量求导」？什么是「矩阵对矩阵求导」？我们学过「向量求导」或者「矩阵求导」吗？</p>
<p>为了消解脑海中尚不清晰的地方，我在搜索引擎中以类似「matrix calculus for engineer」的关键词，找到了一篇长文：<a target="_blank" rel="noopener" href="https://explained.ai/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning</a>，它正好是为像我这样<strong>只记得「单变量求导」的人</strong>量身定做的资料，于是我集中精力花了 3-4 个小时把全文通读，随后在笔记本上将代码中的几个标量 (损失函数)、向量、矩阵间的求导都推了一遍，终于云开雾散，事实证明这样的付出十分值得。</p>
<p>Andrej 在反向传播 Softmax + CrossEntropy 时写了个备注：</p>
<blockquote>
<p>backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</p>
</blockquote>
<p>令人沮丧的是，当你仔细阅读这部分内容，会发现这样一句话：</p>
<blockquote>
<p>It’s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out...</p>
</blockquote>
<p>真正看懂它，是我在阅读完上面那篇关于 Matrix Calculus 的扫盲文章，紧接着继续阅读这篇博客 <a target="_blank" rel="noopener" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">The Softmax function and its derivative</a> 之后。<strong>最终的答案很具有美感，但推导的过程很需要耐心</strong>。</p>
<p>值得一提的是，这里在训练的时候还使用了「teacher forcing」的技巧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dy = np.copy(ps[t])</span><br><span class="line">dy[targets[t]] -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>即不管 RNN 在每次 unroll 都使用标准答案，而不是自身学到的结果 <code>np.argmax(ps[t])</code>，在其它教程中，我也看到过人们会给「teacher forcing」施加一个概率，就像 dropout_p，因为在推理过程中不会有任何字符的参考结果，据说通过这个概率可以一定程度上避免「学生在实际解题时过分依赖老师」。</p>
<h3 id="gradient-check">Gradient Check</h3>
<p>在 min-char-rnn 脚本下的第一个评论，就是 Andrej 自己的：</p>
<figure>
<img src="./gradient-check.png" alt="" /><figcaption>gradient check</figcaption>
</figure>
<p>刚看到这段代码，我完全不知道 gradient check 是在做什么，自然无法理解这段代码的含义。不过在搜索引擎的帮助下，找到了斯坦福大学开放的教程 <a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/">Unsupervised Feature Learning and Deep Learning</a>，其中一节正是介绍 <a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/">Debugging: Gradient Checking</a>。所谓 gradient check(ing) 其实就是对比「通过反向传播计算出来的梯度」与「利用导数定义近似计算得到的梯度」，来判断自己的反向传播阶段代码是否写对了，是工程师构建神经网络时常用的一种简单有效的 debug 方法。由于每次训练都可能非常耗时，而且一些 bug 即便存在，表面上也可能看着一切正常，在构建完神经网络后，开始训练之前，执行一下 gradient check(ing) 很有必要。在实现 2-layer 和 n-layer RNN 的过程中，gradient check(ing) 就成功帮助我发现了代码中的若干逻辑问题，这样的逻辑问题通过传统的「print 调试」、「单点调试」、「眼神调试」都极难发现。</p>
<h2 id="实现-2-layer-和-n-layer-rnn">实现 2-layer 和 n-layer RNN</h2>
<p>2-layer 的 RNN 结构如下图所示：</p>
<figure>
<img src="./two-layer.png" alt="" /><figcaption>The architecture of a 2-layer RNN</figcaption>
</figure>
<p>源码可以在<a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_two_layers.py">这里</a>找到，其中的变量命名与上图的结构一致。</p>
<p>n-layer 的 RNN 结构如下图所示：</p>
<figure>
<img src="./n-layer.png" alt="" /><figcaption>The architecture of a n-layer RNN</figcaption>
</figure>
<p>源码可以在<a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/blob/main/min-char-rnn/min_char_rnn_n_layers.py">这里</a>找到，其中的变量命名与上图的结构一致。</p>
<h2 id="尾声">尾声</h2>
<p>后续我将继续在工作之余，尝试裸写 LSTM 和 GRU，然后逐步复现 Andrej 在七年前完成的其它实验 : )。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy">Github: karpathy</a>, <a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn">char-rnn</a>, <a target="_blank" rel="noopener" href="https://gist.github.com/karpathy/d4dee566867f8291f086">min-char-rnn</a></li>
<li><a target="_blank" rel="noopener" href="https://explained.ai/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">The Softmax function and its derivative</a></li>
<li><a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/">UFLDL Tutorial</a>, <a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/">Debugging: Gradient Checking</a></li>
<li><a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/eliben/deep-learning-samples/blob/master/min-char-rnn/min-char-rnn.py">eliben/deep-learning-samples/min-char-rnn</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZhengHe-MD/replay-nn-tutorials/tree/main/min-char-rnn">ZhengHe-MD/replay-nn-tutorials/min-char-rnn</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/blog/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>
  </div>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blog/2022/02/07/quick-translate/" rel="prev" title="让翻译触手可及">
                  <i class="fa fa-chevron-left"></i> 让翻译触手可及
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZhengHe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/blog/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"zhenghe-hexo-blog","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/blog/js/third-party/comments/disqus.js"></script>

</body>
</html>
